{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asDZDvGrhMyk"
   },
   "source": [
    "# Background: Optimisers in Deep Learning\n",
    "A key component of deep learning is the iterative weight update equation, often called an optimiser. The optimiser is responsible for updating the weights of the network in order to minimise the loss function, and the choice of optimiser can have a significant impact on the performance of the model. In this coursework, you will be implement from scratch some popular optimisers and compare their performance in a simple neural network classifying the MNIST dataset.\n",
    "\n",
    "**Task**: to implement the following optimisers for MNIST classification tasks: **AdaGrad** (Adaptive gradient), **RMSProp** (Root mean square propagation), and **Adam** (Adaptive moments).\n",
    "\n",
    "**Data**: MNIST dataset, which contains 60,000 training images and 10,000 testing images of handwritten digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MueN_DBim2cI"
   },
   "source": [
    "### Optimiser recap\n",
    "We first have a quick recap of the basic ideas in each of the optimisers.  \n",
    "\n",
    "* Stochastic gradient descent (one data point at a time):\n",
    "    $$ \\begin{align*}\n",
    "        w^{(\\tau)} &= w^{(\\tau-1)} + \\Delta w^{(\\tau-1)}, \\\\\n",
    "        \\Delta w^{(\\tau-1)} &= - \\eta \\nabla E_{n}(w^{\\tau-1})\n",
    "    \\end{align*} $$\n",
    "\n",
    "* SGD with momentum:\n",
    "    $$\\begin{align*}\n",
    "        w^{(\\tau)} &= w^{(\\tau-1)} + \\Delta w^{(\\tau-1)}, \\\\\n",
    "        \\Delta w^{(\\tau-1)} &= - \\eta \\left[\\nabla E_{n}(w^{\\tau-1}) + \\mu \\Delta w^{(\\tau-2)}\\right]\n",
    "    \\end{align*} $$\n",
    "    where $\\mu$ ('mu') is called the *momentum parameter*.\n",
    "\n",
    "* AdaGrad (Adaptive gradient): to reduce each learning rate by using the accumulated sum of squared gradients\n",
    "$$    \\begin{align*}\n",
    "        r_i^{(\\tau)} &= r_i^{(\\tau-1)} + \\left(\\frac{\\partial E(w)}{\\partial w_i}\\right)^2, \\\\\n",
    "        w_i^{(\\tau)} &= w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{r_i^{(\\tau)}} + \\epsilon}\\frac{\\partial E(w)}{\\partial w_i}\n",
    "    \\end{align*} $$\n",
    "\n",
    "* RMSProp (Root mean square propagation): moving average of the squared gradient\n",
    "    $$ \\begin{align*}\n",
    "        r_i^{(\\tau)} &= \\beta r_i^{(\\tau-1)} + (1-\\beta) \\left(\\frac{\\partial E(w)}{\\partial w_i}\\right)^2, \\\\\n",
    "        w_i^{(\\tau)} &= w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{r_i^{(\\tau)}} + \\epsilon}\\frac{\\partial E(w)}{\\partial w_i}\n",
    "    \\end{align*}$$\n",
    "    where $\\beta$ ('beta') is called the *decay rate* of the moving average.\n",
    "\n",
    "* Adam (Adaptive moments): moving average for both the gradient and the squared gradient\n",
    "    $$ \\begin{align*}\n",
    "        s_i^{(\\tau)} &= \\beta_1 s_i^{(\\tau-1)} + (1-\\beta_1) \\frac{\\partial E(w)}{\\partial w_i}, \\\\\n",
    "        r_i^{(\\tau)} &= \\beta_2 r_i^{(\\tau-1)} + (1-\\beta_2) \\left(\\frac{\\partial E(w)}{\\partial w_i}\\right)^2 \\\\\n",
    "        \\hat{s_i}^{(\\tau)} &= \\frac{s_i^{(\\tau)}}{1-\\beta_1^{\\tau}}, \\\\\n",
    "        \\hat{r_i}^{(\\tau)} &= \\frac{r_i^{(\\tau)}}{1-\\beta_2^{\\tau}}, \\\\\n",
    "        w_i^{(\\tau)} &= w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{\\hat{r_i}^{(\\tau)}} + \\epsilon}\\hat{s_i}^{(\\tau)}\n",
    "    \\end{align*}$$\n",
    "    where $\\beta_1, \\beta_2$ are the *decay rates* of the moving averages, and with $\\beta_1^{\\tau}, \\beta_2^{\\tau}$ we denote $\\beta_1, \\beta_2$ to the power $\\tau$.\n",
    "\n",
    "In all cases, $\\eta$ is the learning rate, and $\\epsilon$ is a small number to avoid division by zero improve numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exlrRQZIk_F5"
   },
   "source": [
    "### Hyperparameter settings\n",
    "\n",
    "In your implementations, please use the following hyperparameter setups: $\\epsilon=1e-6$, $\\mu=0.9$, $\\eta=0.01$, $\\beta=0.99$, $\\beta_1=0.9$, $\\beta_2=0.99$. (DO NOT CHANGE ANY OF THEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 16873,
     "status": "ok",
     "timestamp": 1730330594717,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "-vMdsMDUnEct"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torch.optim as optim (this is NOT allowed; you can't use torch optimizers)\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "lr = 0.01\n",
    "seed = 10\n",
    "\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda\") or torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLBmCQkEmAGT"
   },
   "source": [
    "and the following convnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730330594717,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "mfid75ZjhJhl"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1) #1个输入通道，32个输出通道，卷积核大小为3x3，步长为1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25) #丢弃率为0.25，用于减少过拟合\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128) #全连接层fc1，输入特征数为9216，输出特征数为128\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #第一个卷积层处理输入数据x\n",
    "        x = F.relu(x) #应用ReLU激活函数，增加非线性\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqJ3hfdvnmSH"
   },
   "source": [
    "We also provide the test function to report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730330594717,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "sHSmarGMml99"
   },
   "outputs": [],
   "source": [
    "# for test accuracy report\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDvTDZW3vP3B"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "MNIST is a well-known dataset of images of handwritten digits. A copy of the MNIST dataset is included in PyTorch. We will download it, then load it into memory using the `torchvision` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4497,
     "status": "ok",
     "timestamp": 1730330599210,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "jZwN_LqPvP3B",
    "outputId": "32fa1300-beea-44b0-b2f3-abc9da78e92f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 9.91M/9.91M [00:01<00:00, 7.86MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 314kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 2.73MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 2.61MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The values 0.1307 and 0.3081 used for the Normalize() transformation below are the global mean and standard\n",
    "# deviation of the MNIST dataset. This is equivalent to scaling all pixel values between [0, 1].\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1730330599993,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "h_H6p1tYvP3B",
    "outputId": "4e014066-ac2d-4b2d-c869-8da14bf71575"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAMWCAYAAACdtUsqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABAJElEQVR4nO3ceZiWddnw8XtGQFEHcHsMVNxxI0XRXDI1wyVTXMqFXNJKzZ7UTJQWM0vNJTUVMreiXNIsFyw1Iddcc8kKEUVKFkdzBcYFUOd+/3iO9+jtec+LmWs8Z+57Zj6fP7/HfVz3T70uhpNLzoZqtVqtAAAAJGms9QEAAICexZABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQKo+7flQa2trpbm5udLU1FRpaGjo7DNBu1Wr1UpLS0tlyJAhlcbG2s7MnhPqWb08K54T6lm9PCeVimeF+tXe56RdQ0Zzc3NljTXWSDscZJszZ05l9dVXr+kZPCd0B7V+VjwndAe1fk4qFc8K9a+t56RdY3pTU1PagaAz1MM9Wg9ngLbU+j6t9fdDe9TDfVoPZ4AlaesebdeQ4TUd9a4e7tF6OAO0pdb3aa2/H9qjHu7TejgDLElb96i/+A0AAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqk+tDwBQ1siRI8P+ta99LeyHHXZY2K+66qqwjx8/PuxPPvlkO04HAHiTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACksl0qyVJLLRX2gQMHply/aGvOsssuG/YNNtgg7P/93/8d9vPOOy/sY8aMCfvChQvDfvbZZ4f9+9//fthhSUaMGBH2KVOmhH3AgAFhr1arYT/00EPDPnr06LCvtNJKYQf+7VOf+lTYr7322rDvuOOOYX/22WfTzgRd4ZRTTgl70e+BGhvjP+vfaaedwn7fffd16Fy14k0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJCq12yXGjp0aNj79esX9u222y7s22+/fdgHDRoU9s9+9rNtH64TzJ07N+wXX3xx2Pfdd9+wt7S0hP2vf/1r2Lvb5gPqw8c+9rGw33jjjWEv2tpWtEWq6D5evHhx2Iu2SG2zzTZhf/LJJ0tdn861ww47hL3ov+vNN9/cmcfpdbbaaquwP/bYY118Eugchx9+eNjHjRsX9tbW1lLXL/pZ1t14kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApOpx26VGjBgR9rvvvjvsRVtquouijQWnnHJK2N96662wX3vttWF/6aWXwv7mm2+G/dlnnw07vcuyyy4b9i222CLs11xzTdgHDx6ccp4ZM2aE/dxzzw379ddfH/YHH3ww7EXP21lnndWO05Ftp512Cvv6668fdtulOqaxMf5zyrXXXjvsa665ZtgbGhrSzgRdoeheXmaZZbr4JPXNmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWP2y41e/bssL/++uthr9V2qUcffTTs8+bNC/snP/nJsC9evDjsV199dYfOBRkuu+yysI8ZM6aLT/I/irZaLb/88mG/7777wl60tWjTTTft0LnoHIcddljYH3744S4+Sc9WtP3tyCOPDHvRFrnp06ennQkyjRo1KuzHHntsqesU3eN77rln2P/1r3+Vun698iYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjV47ZLvfHGG2E/6aSTwl70N/v/8pe/hP3iiy8udZ6nnnoq7LvsskvY33777bBvsskmYT/++ONLnQcyjRw5Muyf+cxnwt7Q0FDq+kVbnn73u9+F/bzzzgt7c3Nz2Iue8zfffDPsO++8c9jL/nPRuRob/flZV7jyyitLfX7GjBmddBL4cLbffvuwT5w4MexlN5P+6Ec/CvusWbNKXae78SsxAACQypABAACkMmQAAACpDBkAAEAqQwYAAJCqx22XKnLLLbeE/e677w57S0tL2DfbbLOwf+lLXwp70baboi1SRZ5++umwH3XUUaWuAx0xYsSIsE+ZMiXsAwYMCHu1Wg37HXfcEfYxY8aEfccddwz7KaecEvaiLTivvvpq2P/617+GvbW1NexF27S22GKLsD/55JNhp5xNN9007KuuumoXn6R3Krthp+jXC6i1L3zhC2EfMmRIqevce++9Yb/qqqvKHqlH8CYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjVa7ZLFVmwYEGpz8+fP7/U54888siw//rXvw570fYa6ArDhg0L+0knnRT2ou0yr732WthfeumlsP/yl78M+1tvvRX22267rVTvbP379w/7iSeeGPaDDz64M4/Ta+yxxx5hL/rvQccUbetae+21S13nxRdfzDgOdNjKK68c9i9+8YthL/o92bx588J+xhlndOhcPZU3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAql6/Xaqs0047LewjR44M+4477hj2UaNGhX3y5MkdOheUsfTSS4f9vPPOC3vRFp+WlpawH3bYYWF//PHHw95TtwENHTq01kfo0TbYYINSn3/66ac76SQ9W9GvC0Vbp5577rmwF/16AdnWWmutsN94440p1x8/fnzY77nnnpTr9xTeZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqWyXKuntt98O+5FHHhn2J598MuxXXHFF2Is2ExRt5fnJT34S9mq1GnaoVCqVzTffPOxFW6SK7L333mG/7777Sp8JOttjjz1W6yN0qQEDBoR99913D/shhxwS9l133bXU955++ulhnzdvXqnrQEcV3eObbrppqevcddddYb/oootKn6k38iYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1SSmTNnhv3www8P+8SJE8N+6KGHlurLLbdc2K+66qqwv/TSS2Gnd7ngggvC3tDQEPaibVG9bYtUY2P85zKtra1dfBI6YsUVV+zU62+22WZhL3quRo0aFfbVV1897P369Qv7wQcfHPai+/Xdd98N+6OPPhr2RYsWhb1Pn/i3EE888UTYIds+++wT9rPPPrvUdR544IGwf+ELXwj7/PnzS12/t/ImAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUJ7v55pvDPmPGjLAXbf351Kc+FfYf/vCHYV9zzTXDfuaZZ4b9xRdfDDvd25577hn2ESNGhL1arYb91ltvzTpSt1a0Raro39tTTz3ViaehaEtS0X+PSy+9NOzf/va3U86z6aabhr1ou9T7778f9nfeeSfs06ZNC/vPf/7zsD/++ONhL9oK969//Svsc+fODXv//v3DPn369LBDR6211lphv/HGG1Ou/49//CPsRc8E7eNNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQynapGpk6dWrYDzjggLDvtddeYZ84cWLYjz766LCvv/76Yd9ll13CTvdWtP2lX79+YX/llVfC/utf/zrtTPVk6aWXDvtpp51W6jp333132L/1rW+VPRIlfPWrXw37rFmzwr7ddtt15nEqs2fPDvstt9wS9meeeSbsjzzySNaRSjnqqKPCvsoqq4S9aCMPZBs3blzYizb+lXX22WenXIf/5E0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqk6M2/evLBfffXVYb/yyivD3qdP/J92hx12CPtOO+0U9nvvvTfs9EyLFi0K+0svvdTFJ8lVtEXqlFNOCftJJ50U9rlz54b9/PPPD/tbb73VjtOR7Zxzzqn1EbqlT33qU6U+f+ONN3bSSeitRowYEfZdd9015fqTJk0K+7PPPptyff6TNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpWpk0003DfvnPve5sG+11VZhL9oiVWTatGlhv//++0tdh57p1ltvrfURPpSizSRF26IOPPDAsBdtIPnsZz/boXNBT3TzzTfX+gj0MJMnTw77CiusUOo6jzzySNgPP/zwskfiQ/AmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUkg022CDsX/va18K+3377hf0jH/lIynk++OCDsL/00kthb21tTfle6ktDQ0Opvs8++4T9+OOPzzpSihNOOCHs3/3ud8M+cODAsF977bVhP+ywwzp2MAA6bKWVVgp72d+jXHLJJWF/6623Sp+JjvMmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUgaItT2PGjAl70RaptdZaK+tIoccffzzsZ555ZthvvfXWzjwOdaZarZbqRff9xRdfHPaf//znYX/99dfDvs0224T90EMPDftmm20W9tVXXz3ss2fPDvudd94Z9qINJMC/FW2jGzZsWNgfeeSRzjwOPcDEiRPD3tiY82ffDz30UMp1+HC8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUvWa7VKrrrpq2DfeeOOwT5gwIewbbrhh2pkijz76aNh/9KMfhX3SpElhb21tTTsTvcdSSy0V9q9+9ath/+xnPxv2BQsWhH399dfv2MH+l6LNIffcc0/YTz311JTvhd6oaBtd1iYgeq4RI0aEfdSoUWEv+r3L4sWLw/6Tn/wk7P/617/aPhydzq8QAABAKkMGAACQypABAACkMmQAAACpDBkAAECqbrtdasUVVwz7ZZddFvaiDQfrrLNO1pFCRVtwzj///LDfeeedYX/33XfTzkTv8fDDD4f9scceC/tWW21V6vof+chHwl60za3I66+/Hvbrr78+7Mcff3yp6wP5tt1227D/4he/6NqDULcGDRoU9qKfHUVefPHFsI8dO7bskehC3mQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKnqZrvU1ltvHfaTTjop7B/72MfCvtpqq6WdKfLOO++E/eKLLw77D3/4w7C//fbbaWeCInPnzg37fvvtF/ajjz467KecckrKeS666KKw//SnPw37888/n/K9QMc1NDTU+ghAN+RNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqm62S+27776lelnTpk0L++9///uwv//++2E///zzwz5v3rwOnQtq4aWXXgr7aaedVqoDPccdd9wR9v3337+LT0JPMX369LA/9NBDYd9+++078zh0MW8yAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUDdVqtdrWhxYsWFAZOHBgV5wHOmT+/PmVAQMG1PQMnhO6g1o/K54TuoNaPyeVimeF+tfWc+JNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqnYNGdVqtbPPAR9KPdyj9XAGaEut79Nafz+0Rz3cp/VwBliStu7Rdg0ZLS0tKYeBzlIP92g9nAHaUuv7tNbfD+1RD/dpPZwBlqSte7Sh2o5RubW1tdLc3FxpamqqNDQ0pB0OPqxqtVppaWmpDBkypNLYWNv/+89zQj2rl2fFc0I9q5fnpFLxrFC/2vuctGvIAAAAaC9/8RsAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUvVpz4daW1srzc3NlaampkpDQ0NnnwnarVqtVlpaWipDhgypNDbWdmb2nFDP6uVZ8ZxQz+rlOalUPCvUr/Y+J+0aMpqbmytrrLFG2uEg25w5cyqrr756Tc/gOaE7qPWz4jmhO6j1c1KpeFaof209J+0a05uamtIOBJ2hHu7RejgDtKXW92mtvx/aox7u03o4AyxJW/dou4YMr+mod/Vwj9bDGaAttb5Pa/390B71cJ/WwxlgSdq6R/3FbwAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUfWp9AADg3y666KKwH3fccWGfOnVq2Pfcc8+wz5o1q2MHAyjBmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJXtUkC309TUFPbll18+7J/5zGfCvsoqq4T9ggsuCPuiRYvacTpon7XWWivshxxySNhbW1vDvtFGG4V9ww03DLvtUnQ3w4YNC3vfvn3DvsMOO4T9kksuCXvRs9XZJk2aFPaDDjoo7IsXL+7M46TzJgMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGW7FFBzRVt2xo0bF/Ztt9027MOHD085z+DBg8N+3HHHpVwfKpVK5dVXXw37/fffH/bRo0d35nGgy2yyySZhP/zww8O+//77h72xMf6z8iFDhoS9aItUtVoNe2creqYvvfTSsH/9618P+4IFC7KOlMqbDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAgle1SSbbeeuuwH3LIIWHfcccdw160caHI2LFjw97c3Bz27bffPuzXXHNN2B999NFS54FKpVLZcMMNw160GePggw8Oe//+/cPe0NAQ9jlz5oS9paUl7BtttFHYDzjggLBfcsklYZ8+fXrYYUnefvvtsM+aNauLTwJd66yzzgr7Hnvs0cUnqU+HHXZY2H/2s5+F/cEHH+zM43SYNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpUo68MADw37RRReFfeWVVw570Xace++9N+yrrLJK2H/0ox+FvUjR9xZd/6CDDip1fXqmgQMHhv2cc84Je9Fz0tTUlHKeGTNmhH233XYLe9++fcNetBWq6Lkt6tARgwYNCvtmm23WtQeBLjZlypSwl90u9corr4S9aAtTY2P8Z+utra2lvne77bYLe9Hm0N7KmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWv3y7Vp0/8r2DLLbcM+xVXXBH2ZZddNuz3339/2E8//fSwP/DAA2Ffeumlw37DDTeEfddddw17kccff7zU5+ld9t1337B/+ctf7tTvnTlzZth32WWXsM+ZMyfs6623XtqZIEvRz42hQ4emXH+rrbYKe9FWtVmzZqV8L7Tlpz/9adhvueWWUtd57733wv7yyy+XPVIpAwYMCPvUqVPDPmTIkFLXL/r30N1+r+ZNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqtdvlzrkkEPCfuWVV5a6zpQpU8J+4IEHhn3BggWlrl90nbJbpObOnRv2X/7yl6WuQ++y//77p1znhRdeCPtjjz0W9nHjxoW9aItUkY022qjU56ErNDc3h/0Xv/hF2E877bRS1y/6/Lx588I+YcKEUteHjnr//ffDXvbX9lrZbbfdwr7CCiukXL/o92qLFi1KuX5X8SYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjVa7ZLnX766WH/9re/HfZqtRr2Sy65JOynnHJK2MtukSryne98J+U6xx13XNhfffXVlOvTMx155JFhP+qoo8I+efLksD///PNhf+WVVzp2sHZaddVVO/X6kKno51XZ7VLAh3PQQQeFvehnYv/+/VO+99RTT025Tq15kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApOpx26WK/kZ+0RapxYsXh/3OO+8M+7hx48L+7rvvtuN0/7bMMsuEfddddw370KFDw97Q0BD2M844I+yTJk1qx+ngPzU3N4e9u2y72XbbbWt9BPjQGhvjPxdsbW3t4pNA93TwwQeH/Zvf/GbY11tvvbD37ds35TxPPfVU2N97772U69eaNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQKpuu11q0KBBYf/qV78a9mq1GvaiLVL77LNPR471/ynaTHDttdeGfeTIkaWu/9vf/jbs5557bqnrQC0dd9xxYV9uueVSrv/Rj3601OcfeuihsD/88MMZx4EOKdoiVfTzDerVWmutFfZDDz007KNGjUr53u233z7sWc/QggULwl60ver2228Pe9mNpfXKmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFW33S7Vr1+/sK+88sqlrlO01ea//uu/wn7EEUeEffTo0WEfPnx42JdffvmwF204KOrXXHNN2N9+++2wQ6Zll1027BtvvHHYv/e974V9jz32KPW9jY3xn48Ubd8p0tzcHPai5/yDDz4odX2A3qzo90C33npr2IcOHdqZx+l0f/rTn8J++eWXd/FJ6oM3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqm67XWrx4sVhf/XVV8O+yiqrhP2f//xn2Iu2OZVVtL1mwYIFYR88eHDYX3vttbD/7ne/69jBINC3b9+wb7755mG/8cYbw150H7/77rthL3pOHn744bDvvvvuYS/adlWkT5/4l8D99tsv7BdddFHYi349AuD/19DQUKpnydpMWGTPPfcM+6c//emw33HHHSnfW6+8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUnXb7VLz5s0L+z777BP23//+92FfccUVwz5z5sywT5o0Key/+MUvwv7GG2+E/frrrw970Vaeos9DR/Tr1y/sRVubbrrpplLX//73vx/2u+++O+wPPvhg2Iuez6LrDB8+vB2n+7eirXNnnXVW2GfPnh32W265JeyLFi0qdR5YkqzNODvssEPYJ0yYUPpMsCRTp04N+0477RT2Qw45JOx33nln2BcuXNihc7XXl770pbAfe+yxnfq9PYU3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqoZqtVpt60MLFiyoDBw4sCvO0+MUbfG47777wl60JeTrX/962MePH9+hc/U08+fPrwwYMKCmZ6jH56Rv375h/8EPfhD2k046qdT177jjjrAfeuihYS/aCle05en2228P+xZbbBH2xYsXh/3cc88Ne9E2qr333jvsRf74xz+G/Zxzzgn7m2++Wer6Tz31VKnPL0mtn5V6fE66iw8++CDs7fgx3i6bbrpp2KdNm5Zy/e6k1s9JpeJZqQdF//5ff/31UtfZa6+9wl70M7S7aOs58SYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjVp9YH6On69+8f9qItUkVbQq6//vq0M9HzLLXUUmE//fTTwz527Niwv/3222H/5je/Gfai+7Joi9SWW24Z9gkTJoR98803D/uMGTPCfswxx4T9nnvuCXvRVoztttsu7AcffHDYR48eHfYpU6aEvcicOXPCvvbaa5e6Dj3TpZdeGvajjz465fpHHXVU2Iu2G0JPt9tuu9X6CN2aNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpTrZnXfeWesj0AsUbYUp2iL1zjvvhL1oS83kyZPDvs0224T9iCOOCPunP/3psBdtYfvBD34Q9okTJ4a9aDtTkQULFoT9D3/4Q6k+ZsyYsH/+858vdZ4TTjih1OfpXaZPn17rI9DL9e3bN+y77rpr2O++++6wv/vuu2lnylD0M+uiiy7q4pP0LN5kAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpGqrVarWtDy1YsKAycODArjhPj7PbbruF/fbbbw970X+OwYMHh/3VV1/t2MF6mPnz51cGDBhQ0zPU8jl56aWXwr7KKquEfdGiRWEv2l6z3HLLhX299dZrx+nadtppp4X9rLPOCvsHH3yQ8r29Ua2fFT9P8j333HNhX3fddUtdp7Ex/nPHoud85syZpa7fndT6OalUavusbL/99mH/zne+E/Zddtkl7GuvvXbYy24CLGvFFVcM+x577BH28ePHh72pqanU9xZtzRo9enTY77nnnlLXrzdtPSfeZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqfrU+gA93TrrrFPrI9ALvPzyy2Ev2i619NJLh32zzTYr9b1FW9Luv//+sN9yyy1hf+GFF8JuixS07emnnw572Z8/ra2tGcehB5gwYULYhw8fXuo6J598cthbWlpKn6mMom1XW2yxRdjbsWj1P9x7771h/+lPfxr27r5FqqO8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUtku1cn+9Kc/hb2xMZ7vbPegI3bYYYew77PPPmEv2rDxyiuvhP3nP/952N98882wL168OOxAvssvvzzse+21VxefBP7TMcccU+sjtEvRz77f/e53YT/++OPDvnDhwrQz9QTeZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqWyX6mRTp04N+4wZM8K+zjrrhH3dddcN+6uvvtqxg9GjtLS0hP3qq68u1YHuZ9q0aWF/5plnwr7RRht15nHoAQ4//PCwH3vssWH/whe+0ImnKTZz5sywv/POO2Ev2vhZtKGt6PdwtI83GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqoZqtVpt60MLFiyoDBw4sCvO02sUbW648sorw37fffeFvWjTQ9G2kZ5q/vz5lQEDBtT0DJ4TuoNaPyueE7qDWj8nlUp9PitLL7102It+T3PGGWeEfYUVVgj7LbfcEvYpU6aEfdKkSWF/+eWXw06utp4TbzIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFS2S9VI0d/Gv+GGG8I+atSosN90001hP+KII8L+9ttvt+N03Y9NINA+tX5WPCd0B7V+TioVzwr1z3YpAACgSxkyAACAVIYMAAAglSEDAABIZcgAAABS9an1AXqrBQsWhP2AAw4I+5lnnhn2Y445JuynnXZa2KdNm9b24QAA4EPwJgMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGW7VJ0p2jp17LHHluoAAFAr3mQAAACpDBkAAEAqQwYAAJDKkAEAAKRq15BRrVY7+xzwodTDPVoPZ4C21Po+rfX3Q3vUw31aD2eAJWnrHm3XkNHS0pJyGOgs9XCP1sMZoC21vk9r/f3QHvVwn9bDGWBJ2rpHG6rtGJVbW1srzc3NlaampkpDQ0Pa4eDDqlarlZaWlsqQIUMqjY21/b//PCfUs3p5Vjwn1LN6eU4qFc8K9au9z0m7hgwAAID28he/AQCAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACBVn/Z8qLW1tdLc3FxpamqqNDQ0dPaZoN2q1WqlpaWlMmTIkEpjY21nZs8J9axenhXPCfWsXp6TSsWzQv1q73PSriGjubm5ssYaa6QdDrLNmTOnsvrqq9f0DJ4TuoNaPyueE7qDWj8nlYpnhfrX1nPSrjG9qakp7UDQGerhHq2HM0Bban2f1vr7oT3q4T6thzPAkrR1j7ZryPCajnpXD/doPZwB2lLr+7TW3w/tUQ/3aT2cAZakrXvUX/wGAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASNWn1gcA6KnuuuuusDc0NIR955137szjUGc23njjsO+5555hP+qoo8L+2GOPhf0vf/lLqfNceOGFYV+8eHGp6wBUKt5kAAAAyQwZAABAKkMGAACQypABAACkMmQAAACpbJdK0rdv37Bvt912Yf/hD38Y9o9//ONpZwK6xo9//OOwFz3/V111VWcehzpz9NFHh/28884L+/LLL1/q+uuuu27YDzrooFLXKdpSdc8995S6DkCl4k0GAACQzJABAACkMmQAAACpDBkAAEAqQwYAAJDKdqkkAwcODHvRVo6XX3457B/5yEdKfR7oOmeffXbYv/KVr4T9vffeC/tdd92Vdibq329+85uw/+AHPwh72e1SWW666aawH3jggWGfPHlyZx4H6Oa8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUtkuVSNFW6Rsl4L6tc0224S9b9++YX/ggQfCfsMNN6Sdifr3xhtvhP173/te2M8///ywL7vssmGfPXt22IcOHdqO0/3boEGDwr777ruH3XYp+HDWXHPNsPfv3z/sY8aMCfsxxxxT6ntvu+22sB9xxBGlrtMWbzIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFS2S9VIQ0NDrY8AXW6HHXYI+3e+852wF23SKNrWk6Xoe4cPHx72mTNnhn3s2LFpZ6LnufTSS8P+la98JeybbbZZ2BcsWJB2psiECRM69frQU4waNSrs++23X9iLftYMHDgw7NVqtWMH+1+KNiVm8yYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1SNFG0IWGaZZbr4JNB1Lr/88rCvv/76Yd94443D/sADD6SdKfLtb3877CuttFLYjzzyyLD/9a9/TTsTvccZZ5wR9qItbCNGjOjE01Qq/fr169TrQ7268sorw/7Rj3407FtttVXK97a0tIT92muvDftjjz0W9uuuuy7sCxcu7NjBSvImAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUndlyyy3D/sgjj3TxSSDfO++8E/ZabVsr2sqz5pprhr21tTXstsKR6be//W3Yi7aqTZ48OexFG3DKKtp29bnPfS7l+tBVijYEnnXWWWH/4he/GPY33ngj7E888UTYzz777LBPnTo17O+++27YZ8+eHfZ65U0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqkk77//ftjnz58f9oEDB4Z93XXXTTsT1Mrpp58e9qJtN88880zY//rXv6acZ7nllgv7uHHjwr7sssuGvWjLW9E2IOiIgw8+OOybbbZZ2IcPH96ZxyncagXdzXe/+92wf+lLXwr7+PHjw/6d73wn7G+99VbHDtZDeZMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKSyXSrJvHnzwv6nP/0p7HvuuWcnnga6xhprrBH2I488MuxFW9i+9rWvhf3VV1/t2MH+lwsuuCDs+++/f9ibm5vD/vGPfzzlPPQuG264YdhvvvnmsK+33nph79OnNj+yb7311pp8L/xfRRv/ijYEHnrooWH/+te/HvZ77rkn7HfeeWfYFy5cGHb+kzcZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAq26WANg0fPjzsRdtxVl555bCPHz8+7Pfdd1/HDva/jB07NuyHH354qeuceeaZCaeB/7HRRhuFfe211w57rbZIFTnhhBPCfuyxx3bxSeitTjnllLAXbZe64YYbwj558uSw2xbVObzJAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABS1dcKCyorrbRSrY9AL1C0veaQQw4J+89+9rOwNzbGf07R2toa9m233Tbs3/rWt8J+wQUXhH3FFVcM+/777x/2hoaGsF911VVhv+yyy8IOHVG0he3kk08O+znnnBP2ZZZZJu1MZQwePLgm3wv/V9HPiGq1Gvbrrrsu7LZIdS1vMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVLZL1ZnRo0fX+gj0AgcddFDYr7zyyrAXbfAo2iL1/PPPh33LLbcs1ffee++wr7baamEv2oLz6quvhv2LX/xi2KErXHzxxWGfMWNG2AcNGlTq+kVb5CZMmBD2AQMGlLo+dJU///nPYS/62VF0j7/77rthnzJlSscOxhJ5kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApLJdqpPdc889Yd9zzz27+CT0RgceeGDYJ06cGPb33nsv7PPmzQv75z//+bC/+eabYT///PPDvuOOO4a9aHNIQ0ND2Iu2YK288sphnzNnTth32mmnsM+cOTPskOmOO+5IuU7Rc7LeeuuF/dRTTw37iBEjwr7mmmuGfdasWW0fjl5h6623Dvtf/vKXsC9evDjsn/70p8N+3HHHhf273/1u2H/729+Gveic06dPDzvt400GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqlONnv27FKf79u3b9ht8aAjjj766LAX3ZdnnHFG2Iu2UZV17LHHhv2yyy4L+7bbbpvyvUVbdoq2v9kiRU/Qr1+/sBdtkSpStHXugw8+KH0murfBgweH/fe//33Yhw4dGvYTTjgh7Ndcc03Y33jjjbBPmDAh7EXbpZZffvmwr7jiimHnw/EmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUJ3v//fdLfb5oC87SSy+dcRx6mUmTJoX9pptuCvucOXM68ziVlVdeOezDhw8vdZ0xY8aEferUqaWuM3fu3FKfh+6kaFtcWT/72c/C7vnpfZ588smwDxgwIOzjxo0Le9EWqbKOP/74Up//4x//GPayPztoH28yAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUDdVqtdrWhxYsWFAZOHBgV5yn15g2bVrYN9xww7BfeumlYf/qV7+adqbubP78+YXbLbqK5+Tfiv49FG27KbqPZ86cGfZhw4Z17GDU/Fmpx+dkpZVWCvvEiRPDft1115XqnW3w4MFhnz59etjL/vdfd911w/6Pf/yj1HW6k1o/J5VKfT4r3/rWt8J+yimnhL1///4p3ztjxoywr7/++mGfNWtW2D/72c+GvWhrFkvW1nPiTQYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkKpPrQ/QW02ePDnsq622Wti/8Y1vdOZxIFXRtqhjjjkm7K+88krYd95557QzQZGLL7447HvttVfYi7abNTc3h/3FF18M+/PPPx/2kSNHlvrek08+OexltyOdf/75YS/656L3Oeuss8L+3nvvhX3zzTcP+6hRo0p97worrBD22267Lexjx44Ne9EzR+fwJgMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGW7VJ2pVqthX7x4cRefBNq25pprhv3LX/5y2Ivu78svvzzsc+fO7djBoITx48eHfe211w77tttuG/Z777037C+88ELYp02bFvZPfOITYW9qagp7kaLnbfr06WH/3ve+F/aFCxeW+l56n/POO6/WR6AOeZMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKSyXarODBgwIOx777132G+++ebOPA4s0ZQpU8JetHXqmmuuCXvRVhvoCo888kjYH3744bBfffXVYb/kkkvCvtZaa5XqWd58882wb7zxxp36vQCVijcZAABAMkMGAACQypABAACkMmQAAACpDBkAAEAq26Vq5IADDgj7okWLwv7MM8905nGgQyZOnBj2008/PeyTJk3qzONAqhNPPDHsSy+9dNiXX375UtfffPPNwz5mzJhS15k/f37Yd9lll1LXAcjkTQYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkKqhWq1W2/rQggULKgMHDuyK8/Qa119/fdg32mijsI8ePTrss2bNSjtTdzZ//vzKgAEDanoGzwndQa2fFc8J3UGtn5NKxbNC/WvrOfEmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABI1afWB+itDjrooFofAQAAOoU3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqdo1ZFSr1c4+B3wo9XCP1sMZoC21vk9r/f3QHvVwn9bDGWBJ2rpH2zVktLS0pBwGOks93KP1cAZoS63v01p/P7RHPdyn9XAGWJK27tGGajtG5dbW1kpzc3Olqamp0tDQkHY4+LCq1WqlpaWlMmTIkEpjY23/7z/PCfWsXp4Vzwn1rF6ek0rFs0L9au9z0q4hAwAAoL38xW8AACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASNWnPR9qbW2tNDc3V5qamioNDQ2dfSZot2q1WmlpaakMGTKk0thY25nZc0I9q5dnxXNCPauX56RS8axQv9r7nLRryGhubq6sscYaaYeDbHPmzKmsvvrqNT2D54TuoNbPiueE7qDWz0ml4lmh/rX1nLRrTG9qako7EHSGerhH6+EM0JZa36e1/n5oj3q4T+vhDLAkbd2j7RoyvKaj3tXDPVoPZ4C21Po+rfX3Q3vUw31aD2eAJWnrHvUXvwEAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABS9an1AQCyDBs2LOx/+MMfwr7UUkuFfc0110w7EwD0Rt5kAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpbJcCup3x48eH/cADDwz7iiuuGPbf//73aWcCAP7NmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJXtUkDNrbrqqmG/6aabwr7NNtuEvVqthn3q1Klh/9KXvtSO0wEAZXmTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACk6vLtUssvv3zYDzzwwLAvXLgw7CNHjgx7U1NT2A8++OCw33vvvWF/8cUXw57l5ZdfDvukSZPC/vjjj3fmcaBLDBs2LOznnXde2LfeeutS1//Wt74V9qLn5/XXXy91fcjU0NAQ9uuuuy7se+yxR9g33njjsM+dO7djBwNI4E0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJCqy7dLnXrqqWEfO3ZsF5/kf+y+++41+d4iRdtxpk2bFvaiLSRF/YUXXujQuSDDiiuuGPairTllFW3Tueeee1KuD5n69+8f9o9//ONhL9rOWPRz7Morr+zYwQASeJMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKTq8u1S++23X6de//XXXw/73/72t0793meffTbsG2ywQdgHDRoU9s033zzsw4cPD/uZZ54Z9qJ/Xtul6ArDhg0L+69+9auwNzQ0lLp+0a8jkyZNKnUdqKV33nkn7DNmzAj7aqutFvZVVlkl7UzQG5144olh79evX9g32mijsB988MGlvnf69Olh32STTUpdp155kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApOry7VK77bZb2Iu20Tz33HOlrl+0reOll14qdZ3O1tTUFPa///3vYR86dGip648ePTrst912W6nrQEcceuihYS+6j2+//fawf+UrXwn7iy++2LGDQTfwk5/8JOw77bRT2Is23UBPt+OOO4a9aCNn0ef33XffsJfdfFitVkt9fv311w/7tGnTwr7xxhuXun6teZMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKTq8u1SM2fOLNV7qj333DPsZbdILVq0KOxXXHFF6TNBWQ899FDYR4wYEfYXXngh7CeccELYbZGiN/rzn/9c6vMHHHBA2MeNGxf2etu2SM81ePDgsF933XVhX2eddUpdf+DAgWFfbrnlwl60LeqJJ54I+xZbbFHqPGU1NsZ/1l90/u7GmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWXb5fqqfr16xf2iy++OOyHHXZYyvduu+22YX/qqadSrg+VSqWy9957h33rrbcOe7VaDftvfvObsC9cuLBjB4NepGgzTtHPn9GjR4f9sssuSzsTVCqVyqhRo8JetOlyjTXW6MzjFNp4443D/tprr4V95ZVXDvuQIUPCPnHixLCvvvrq7Tjdv02bNq3U5+uVNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpUr65Cc/GfZDDz007Icffnip67/33nthP+6448I+ffr0UteHJRk0aFDYP/GJT6Rc/8033wz73LlzU65f5Pjjjw972Q0nY8eOzTgOdEjR1rYiRVunINvJJ58c9qwtUosWLQr7uHHjwv7II4+E/dlnny31va+//nrYi36mlN0i9cILL4S96PeU3Y03GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKtulCnzsYx8L++TJk8O+1FJLpXxv0faQ2bNnh/2DDz5I+V6oVIrvp5EjR4a9sTH+c4rW1taw33///R072P9ywgknlPr8scceG/Y111yz1HVOPPHEsBdtFHnxxRdLXR+gnu26665h32abbVKuX/R7naJtSw8++GDK95ZVdotUkUmTJoX9tddeS7l+rXmTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACksl2qwAEHHBD2rC1SRfr16xf22267LeyPP/542H/3u9+F/eabbw771KlT23E6erodd9wx7J/4xCfCXrRFqmhDSNmNGSNGjCh1ntGjR5e6/ttvvx32uXPnhn2DDTYI+29/+9uwH3TQQWGfNWtWO04HUF+KNuwtu+yypa7z0EMPhf373/9+2Dt7i9QKK6wQ9t133z3sO+ywQ6nrF/3z3n777aWu0914kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApLJdqsBNN90U9o022ijsW221VdhXXnnltDNFttxyy1L9e9/7XtgvvPDCsJ977rlhf+WVV9o+HHWrqakp7GuvvXap6zQ3N4f96quvDvvzzz8f9mHDhoX9pJNOCvvee+8d9qLtVZMnTw77+eefH/aBAweG/e677y71ecjU0NAQ9mq12sUnobe6/PLLw170e5358+eH/fOf/3zYX3755Y4d7EP6yle+EvbTTz+91HWefvrpsBdtLK3VP29X8SYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1SBhx56KOyf+cxnwj506NCwF21cWHXVVcO+3377hf2LX/xi2Iu2jRRpbIznym984xthHzlyZNg/9alPhb21tbXUeaiN7bffPuw//vGPS13niiuuCPsPfvCDsBfd9+edd17Y99hjj7C3tLSE/YYbbgj72LFjw77++uuH/dJLLy31vXfddVfYZ82aFXboCFukqLUbb7yxVK83e+21V9hPPfXUUtd5//33w170s6Onb5Eq4k0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqkks2fPLtWL3HHHHWG/9957w37ssceG/WMf+1ip7y2y4447hr1oW8+5556b8r10rk033TTlOkVbpIrcdNNNYd96661LXWfvvfcO+3333Rf2bbbZJuwPPPBAqe+98MILw170PEAt/e1vf6v1EaCu3HLLLWEvu7ntuOOOC/vll19e9kg9mjcZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAq26W6iWuvvTbsv/71r8P+xz/+Mew77LBDynnWW2+9lOtQG4MGDQp7Q0ND2CdNmlTq+iNGjAj7WmutVep7TzzxxLAXbZEaNmxY2H/1q1+lfG/RdimoRzNnzqz1EaAmfvjDH4a9sTH+s/XW1tZS1y/6GcR/8iYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1Q39/7774f9iSeeCHvWdqnnnnsu5TrUl2q1WqqXVbTBo+j6m266adhnz54d9mWWWSbs//znP8P+iU98Iuzz588POwD1o1+/fmHffPPNw172Z9Dxxx8f9hkzZrTjdHiTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACk6jXbpQYPHhz2I488MuzTp08P+w033JB2pgxLLbVU2DfbbLOU6xdtr3rkkUdSrk9tTJo0KewnnXRS2Pfee++wb7PNNmEfMWJE2Juamto+3P/jsMMOC3tDQ0PYX3vttbCfdtppYX/xxRdLnQe6k6WXXrrWR4AUyy67bNgPOeSQsO+yyy6lrn/dddeF/dprrw170ZYq/pM3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqh63XeojH/lI2P/whz+E/aMf/WjYV1hhhbQzZVh11VXD/o1vfCPsO++8c8r3PvPMM2F/4IEHUq5Pbbz33nthf+edd8JetNnjwQcfDHu1Wu3YwdqppaUl7EXb3+64447OPA7UpT322CPs48eP7+KTQPsUbSC84oorwv65z32u1PVPOOGEsE+YMCHstkh9ON5kAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpetx2qQsvvDDsRVukiqy99tphf/bZZ8P+7rvvlrp+//79w37yySeHvWiLVNEmhiINDQ1hL9rWc9xxx5W6Pt3DE088EfYxY8aEvej+22mnnVLO88tf/jLsf//738P+l7/8Jez33Xdfynmglv71r3+F/emnnw77Jpts0pnHgS6z2mqrhb3sFqmZM2eG/eKLLy59JjrOmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWP2y511113hf2AAw4odZ0nn3wy7EVbbebPn1/q+gMHDgz75ptvXuo6ZRVtkdp3333DbltP73LbbbeV6kC+xYsXh33hwoWlrrPLLruEffz48aXPBJk23HDDsJ944omlrvPcc8+F/dOf/nTpM5HPmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWP2y41ZcqUsF9//fVhP+igg0pdv7O3P5X1/vvvh/3CCy8M+4033hj2Rx99NOtIAHSCp556KuwjR44M+/LLL9+Jp4GO++53vxv2Aw88sNR1ijalzZo1q/SZyOdNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqsdtl3rhhRfCfsQRR4T91ltvDfvOO+8c9ueeey7so0ePbvtw/4/p06eX+vzdd99d6jpFW0gA6J7OPPPMsA8fPjzsN9xwQ2ceB9q0ySabhH3AgAGlrnP55ZeHvej3RtQHbzIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFQN1Wq12taHFixYUBk4cGBXnAc6ZP78+aW3VWTznNAd1PpZ8ZzQHdT6OalUesazcs4554T9xBNPDPusWbPCvscee4T92Wef7djBSNHWc+JNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqk+tDwAAQM8zefLksBdtl/rGN74RdlukuidvMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVLZLAQCQ7q677gp7nz5++9kbeJMBAACkMmQAAACpDBkAAEAqQwYAAJCqXUNGtVrt7HPAh1IP92g9nAHaUuv7tNbfD+1RD/dpPZwBlqSte7RdQ0ZLS0vKYaCz1MM9Wg9ngLbU+j6t9fdDe9TDfVoPZ4Alaesebai2Y1RubW2tNDc3V5qamioNDQ1ph4MPq1qtVlpaWipDhgypNDbW9v/+85xQz+rlWfGcUM/q5TmpVDwr1K/2PiftGjIAAADay1/8BgAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACDV/wGPUF/H+0rWkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualize the first 16 elements of the training set\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.imshow(dataset1.data[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYBp0zPEoCkj"
   },
   "source": [
    "# Implementations (7 marks)\n",
    "In PyTorch, optimisers are created by passing the model parameters and hyperparameters to the optimiser class. The optimiser is then used to update the model parameters after each forward pass. Following this convension, we will write our optimisers as *classes*, an example structure for which is given below. Feel free to add whatever parameters or class functions that you see fit.\n",
    "\n",
    "Implement the optimisers **AdaGrad** (Adaptive gradient), **RMSProp** (Root mean square propagation), and **Adam** (Adaptive moments) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730330599993,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "DH9VlFe5vP3C"
   },
   "outputs": [],
   "source": [
    "#adagrad_optimizer\n",
    "class adagrad_optimizer:\n",
    "\n",
    "     # Initialize the model, learning rate, and a small value for numerical stability 初始化\n",
    "    def __init__(self, model, lr=0.01, epsilon=1e-6):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.accumulated_grad_squares = [torch.zeros_like(p) for p in model.parameters()]\n",
    "\n",
    "     # Update each parameter using the Adagrad algorithm 更新参数\n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "            if param.grad is not None:\n",
    "                self.accumulated_grad_squares[i] += param.grad ** 2\n",
    "                adjustment = self.lr / (self.accumulated_grad_squares[i].sqrt() + self.epsilon)\n",
    "                param.data -= adjustment * param.grad\n",
    "\n",
    "    # Zero the gradients for the next iteration 为下一次迭代将梯度归零\n",
    "    def zero_grad(self):\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "#rmsprop_optimizer\n",
    "class rmsprop_optimizer:\n",
    "\n",
    "    # Initialize the model, learning rate, and a small value for numerical stability 初始化\n",
    "    def __init__(self, model, lr=0.01, rho=0.99, epsilon=1e-6):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.accumulated_grad_squares = [torch.zeros_like(p) for p in model.parameters()]\n",
    "\n",
    "    # Update each parameter using the Adagrad algorithm 更新参数\n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "            if param.grad is not None:\n",
    "                self.accumulated_grad_squares[i] = self.rho * self.accumulated_grad_squares[i] + (1 - self.rho) * (param.grad ** 2)\n",
    "                adjustment = self.lr / (self.accumulated_grad_squares[i].sqrt() + self.epsilon)\n",
    "                param.data -= adjustment * param.grad\n",
    "\n",
    "    # Zero the gradients for the next iteration 为下一次迭代将梯度归零\n",
    "    def zero_grad(self):\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "#####################################################################################################\n",
    "# adam_optimizer\n",
    "class adam_optimizer:\n",
    "\n",
    "    # Initialize the model, learning rate, and a small value for numerical stability 初始化\n",
    "    def __init__(self, model, lr=0.01, beta1=0.9, beta2=0.99, epsilon=1e-6):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = [torch.zeros_like(p) for p in model.parameters()]  # First moment estimation 一阶矩估计\n",
    "        self.v = [torch.zeros_like(p) for p in model.parameters()]  # Second moment estimation 二阶矩估计\n",
    "        self.t = 0\n",
    "\n",
    "    # Update each parameter using the Adagrad algorithm 更新参数\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.model.parameters()):\n",
    "            if param.grad is not None:\n",
    "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * param.grad\n",
    "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (param.grad ** 2)\n",
    "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "                param.data -= self.lr * m_hat / (v_hat.sqrt() + self.epsilon)\n",
    "\n",
    "    # Zero the gradients for the next iteration 为下一次迭代将梯度归零\n",
    "    def zero_grad(self):\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730330599994,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "mCSP_SEcmi7L"
   },
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, test_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        ########################################\n",
    "        # TODO\n",
    "        # put your optimizer implementations here. Feel free to move your codes around\n",
    "        # HINTS: You can use loss.backward() to compute gradients and then access the\n",
    "        # gradients in your functions via [p.grad.data for p in model.parameters()].\n",
    "        # Or you can compute the gradients by calling the torch.autograd.grad function.\n",
    "\n",
    "        loss.backward()  # Computed gradients via backpropagation反向传播计算梯度\n",
    "\n",
    "        optimizer.step()  # Updated parameter更新参数\n",
    "        ########################################\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train: [{batch_idx * len(data)}/{len(train_loader.dataset)}({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "            test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id8H08tloPTr"
   },
   "source": [
    "# Model training and submission (3 marks)\n",
    "\n",
    "Use the following training settings for your model training. You may have to run this code multiple times, or create a loop, to train the model with different optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 683884,
     "status": "ok",
     "timestamp": 1730331283876,
     "user": {
      "displayName": "Qiaoqiao Zhou",
      "userId": "03835980260175001750"
     },
     "user_tz": 0
    },
    "id": "cTPITptmmv09",
    "outputId": "07a22742-0dea-45c1-ce3f-8f1a77affe40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with adagrad optimizer:\n",
      "Train: [0/60000(0%)]\tLoss: 2.304011\n",
      "\n",
      "Test set: Average loss: 14.6498, Accuracy: 1479/10000 (15%)\n",
      "\n",
      "Train: [640/60000(1%)]\tLoss: 1.544461\n",
      "\n",
      "Test set: Average loss: 1.2862, Accuracy: 6157/10000 (62%)\n",
      "\n",
      "Train: [1280/60000(2%)]\tLoss: 0.302985\n",
      "\n",
      "Test set: Average loss: 0.4050, Accuracy: 8808/10000 (88%)\n",
      "\n",
      "Train: [1920/60000(3%)]\tLoss: 0.272422\n",
      "\n",
      "Test set: Average loss: 0.2878, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Train: [2560/60000(4%)]\tLoss: 0.455804\n",
      "\n",
      "Test set: Average loss: 0.2712, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Train: [3200/60000(5%)]\tLoss: 0.057196\n",
      "\n",
      "Test set: Average loss: 0.2263, Accuracy: 9349/10000 (93%)\n",
      "\n",
      "Train: [3840/60000(6%)]\tLoss: 0.175752\n",
      "\n",
      "Test set: Average loss: 0.2061, Accuracy: 9363/10000 (94%)\n",
      "\n",
      "Train: [4480/60000(7%)]\tLoss: 0.145704\n",
      "\n",
      "Test set: Average loss: 0.2034, Accuracy: 9434/10000 (94%)\n",
      "\n",
      "Train: [5120/60000(9%)]\tLoss: 0.203875\n",
      "\n",
      "Test set: Average loss: 0.1763, Accuracy: 9465/10000 (95%)\n",
      "\n",
      "Train: [5760/60000(10%)]\tLoss: 0.283379\n",
      "\n",
      "Test set: Average loss: 0.1507, Accuracy: 9589/10000 (96%)\n",
      "\n",
      "Train: [6400/60000(11%)]\tLoss: 0.196012\n",
      "\n",
      "Test set: Average loss: 0.1620, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "Train: [7040/60000(12%)]\tLoss: 0.167961\n",
      "\n",
      "Test set: Average loss: 0.1517, Accuracy: 9561/10000 (96%)\n",
      "\n",
      "Train: [7680/60000(13%)]\tLoss: 0.074622\n",
      "\n",
      "Test set: Average loss: 0.1358, Accuracy: 9596/10000 (96%)\n",
      "\n",
      "Train: [8320/60000(14%)]\tLoss: 0.083929\n",
      "\n",
      "Test set: Average loss: 0.1240, Accuracy: 9636/10000 (96%)\n",
      "\n",
      "Train: [8960/60000(15%)]\tLoss: 0.045602\n",
      "\n",
      "Test set: Average loss: 0.1261, Accuracy: 9624/10000 (96%)\n",
      "\n",
      "Train: [9600/60000(16%)]\tLoss: 0.120641\n",
      "\n",
      "Test set: Average loss: 0.1248, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "Train: [10240/60000(17%)]\tLoss: 0.191737\n",
      "\n",
      "Test set: Average loss: 0.1124, Accuracy: 9676/10000 (97%)\n",
      "\n",
      "Train: [10880/60000(18%)]\tLoss: 0.092307\n",
      "\n",
      "Test set: Average loss: 0.1068, Accuracy: 9682/10000 (97%)\n",
      "\n",
      "Train: [11520/60000(19%)]\tLoss: 0.287731\n",
      "\n",
      "Test set: Average loss: 0.1190, Accuracy: 9629/10000 (96%)\n",
      "\n",
      "Train: [12160/60000(20%)]\tLoss: 0.057098\n",
      "\n",
      "Test set: Average loss: 0.1028, Accuracy: 9678/10000 (97%)\n",
      "\n",
      "Train: [12800/60000(21%)]\tLoss: 0.151818\n",
      "\n",
      "Test set: Average loss: 0.1062, Accuracy: 9712/10000 (97%)\n",
      "\n",
      "Train: [13440/60000(22%)]\tLoss: 0.104376\n",
      "\n",
      "Test set: Average loss: 0.0979, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train: [14080/60000(23%)]\tLoss: 0.044304\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train: [14720/60000(25%)]\tLoss: 0.018125\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train: [15360/60000(26%)]\tLoss: 0.055255\n",
      "\n",
      "Test set: Average loss: 0.0913, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train: [16000/60000(27%)]\tLoss: 0.130419\n",
      "\n",
      "Test set: Average loss: 0.0887, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train: [16640/60000(28%)]\tLoss: 0.114999\n",
      "\n",
      "Test set: Average loss: 0.0851, Accuracy: 9747/10000 (97%)\n",
      "\n",
      "Train: [17280/60000(29%)]\tLoss: 0.095927\n",
      "\n",
      "Test set: Average loss: 0.0802, Accuracy: 9760/10000 (98%)\n",
      "\n",
      "Train: [17920/60000(30%)]\tLoss: 0.029049\n",
      "\n",
      "Test set: Average loss: 0.0795, Accuracy: 9771/10000 (98%)\n",
      "\n",
      "Train: [18560/60000(31%)]\tLoss: 0.050340\n",
      "\n",
      "Test set: Average loss: 0.0809, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Train: [19200/60000(32%)]\tLoss: 0.084526\n",
      "\n",
      "Test set: Average loss: 0.0762, Accuracy: 9767/10000 (98%)\n",
      "\n",
      "Train: [19840/60000(33%)]\tLoss: 0.069662\n",
      "\n",
      "Test set: Average loss: 0.0801, Accuracy: 9755/10000 (98%)\n",
      "\n",
      "Train: [20480/60000(34%)]\tLoss: 0.083388\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9777/10000 (98%)\n",
      "\n",
      "Train: [21120/60000(35%)]\tLoss: 0.097060\n",
      "\n",
      "Test set: Average loss: 0.0752, Accuracy: 9774/10000 (98%)\n",
      "\n",
      "Train: [21760/60000(36%)]\tLoss: 0.038098\n",
      "\n",
      "Test set: Average loss: 0.0785, Accuracy: 9765/10000 (98%)\n",
      "\n",
      "Train: [22400/60000(37%)]\tLoss: 0.081102\n",
      "\n",
      "Test set: Average loss: 0.0735, Accuracy: 9781/10000 (98%)\n",
      "\n",
      "Train: [23040/60000(38%)]\tLoss: 0.255498\n",
      "\n",
      "Test set: Average loss: 0.0794, Accuracy: 9763/10000 (98%)\n",
      "\n",
      "Train: [23680/60000(39%)]\tLoss: 0.063285\n",
      "\n",
      "Test set: Average loss: 0.0691, Accuracy: 9786/10000 (98%)\n",
      "\n",
      "Train: [24320/60000(41%)]\tLoss: 0.063953\n",
      "\n",
      "Test set: Average loss: 0.0713, Accuracy: 9794/10000 (98%)\n",
      "\n",
      "Train: [24960/60000(42%)]\tLoss: 0.091786\n",
      "\n",
      "Test set: Average loss: 0.0827, Accuracy: 9743/10000 (97%)\n",
      "\n",
      "Train: [25600/60000(43%)]\tLoss: 0.038606\n",
      "\n",
      "Test set: Average loss: 0.0672, Accuracy: 9797/10000 (98%)\n",
      "\n",
      "Train: [26240/60000(44%)]\tLoss: 0.034848\n",
      "\n",
      "Test set: Average loss: 0.0615, Accuracy: 9819/10000 (98%)\n",
      "\n",
      "Train: [26880/60000(45%)]\tLoss: 0.033419\n",
      "\n",
      "Test set: Average loss: 0.0627, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "Train: [27520/60000(46%)]\tLoss: 0.106580\n",
      "\n",
      "Test set: Average loss: 0.0675, Accuracy: 9812/10000 (98%)\n",
      "\n",
      "Train: [28160/60000(47%)]\tLoss: 0.088020\n",
      "\n",
      "Test set: Average loss: 0.0624, Accuracy: 9821/10000 (98%)\n",
      "\n",
      "Train: [28800/60000(48%)]\tLoss: 0.071776\n",
      "\n",
      "Test set: Average loss: 0.0636, Accuracy: 9807/10000 (98%)\n",
      "\n",
      "Train: [29440/60000(49%)]\tLoss: 0.010817\n",
      "\n",
      "Test set: Average loss: 0.0602, Accuracy: 9823/10000 (98%)\n",
      "\n",
      "Train: [30080/60000(50%)]\tLoss: 0.175879\n",
      "\n",
      "Test set: Average loss: 0.0644, Accuracy: 9811/10000 (98%)\n",
      "\n",
      "Train: [30720/60000(51%)]\tLoss: 0.159435\n",
      "\n",
      "Test set: Average loss: 0.0671, Accuracy: 9792/10000 (98%)\n",
      "\n",
      "Train: [31360/60000(52%)]\tLoss: 0.040740\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 9812/10000 (98%)\n",
      "\n",
      "Train: [32000/60000(53%)]\tLoss: 0.039980\n",
      "\n",
      "Test set: Average loss: 0.0624, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "Train: [32640/60000(54%)]\tLoss: 0.035077\n",
      "\n",
      "Test set: Average loss: 0.0566, Accuracy: 9826/10000 (98%)\n",
      "\n",
      "Train: [33280/60000(55%)]\tLoss: 0.182922\n",
      "\n",
      "Test set: Average loss: 0.0679, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Train: [33920/60000(57%)]\tLoss: 0.029781\n",
      "\n",
      "Test set: Average loss: 0.0625, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "Train: [34560/60000(58%)]\tLoss: 0.049775\n",
      "\n",
      "Test set: Average loss: 0.0571, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Train: [35200/60000(59%)]\tLoss: 0.080328\n",
      "\n",
      "Test set: Average loss: 0.0622, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Train: [35840/60000(60%)]\tLoss: 0.047635\n",
      "\n",
      "Test set: Average loss: 0.0602, Accuracy: 9803/10000 (98%)\n",
      "\n",
      "Train: [36480/60000(61%)]\tLoss: 0.071156\n",
      "\n",
      "Test set: Average loss: 0.0627, Accuracy: 9810/10000 (98%)\n",
      "\n",
      "Train: [37120/60000(62%)]\tLoss: 0.039944\n",
      "\n",
      "Test set: Average loss: 0.0566, Accuracy: 9823/10000 (98%)\n",
      "\n",
      "Train: [37760/60000(63%)]\tLoss: 0.077620\n",
      "\n",
      "Test set: Average loss: 0.0535, Accuracy: 9833/10000 (98%)\n",
      "\n",
      "Train: [38400/60000(64%)]\tLoss: 0.020180\n",
      "\n",
      "Test set: Average loss: 0.0569, Accuracy: 9825/10000 (98%)\n",
      "\n",
      "Train: [39040/60000(65%)]\tLoss: 0.045261\n",
      "\n",
      "Test set: Average loss: 0.0540, Accuracy: 9832/10000 (98%)\n",
      "\n",
      "Train: [39680/60000(66%)]\tLoss: 0.079529\n",
      "\n",
      "Test set: Average loss: 0.0583, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "Train: [40320/60000(67%)]\tLoss: 0.104264\n",
      "\n",
      "Test set: Average loss: 0.0599, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "Train: [40960/60000(68%)]\tLoss: 0.083484\n",
      "\n",
      "Test set: Average loss: 0.0556, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Train: [41600/60000(69%)]\tLoss: 0.032353\n",
      "\n",
      "Test set: Average loss: 0.0522, Accuracy: 9833/10000 (98%)\n",
      "\n",
      "Train: [42240/60000(70%)]\tLoss: 0.081617\n",
      "\n",
      "Test set: Average loss: 0.0543, Accuracy: 9829/10000 (98%)\n",
      "\n",
      "Train: [42880/60000(71%)]\tLoss: 0.066586\n",
      "\n",
      "Test set: Average loss: 0.0548, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Train: [43520/60000(72%)]\tLoss: 0.116861\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9817/10000 (98%)\n",
      "\n",
      "Train: [44160/60000(74%)]\tLoss: 0.101329\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "Train: [44800/60000(75%)]\tLoss: 0.005255\n",
      "\n",
      "Test set: Average loss: 0.0553, Accuracy: 9819/10000 (98%)\n",
      "\n",
      "Train: [45440/60000(76%)]\tLoss: 0.067383\n",
      "\n",
      "Test set: Average loss: 0.0539, Accuracy: 9826/10000 (98%)\n",
      "\n",
      "Train: [46080/60000(77%)]\tLoss: 0.011256\n",
      "\n",
      "Test set: Average loss: 0.0513, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train: [46720/60000(78%)]\tLoss: 0.027851\n",
      "\n",
      "Test set: Average loss: 0.0538, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train: [47360/60000(79%)]\tLoss: 0.192526\n",
      "\n",
      "Test set: Average loss: 0.0545, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "Train: [48000/60000(80%)]\tLoss: 0.101909\n",
      "\n",
      "Test set: Average loss: 0.0496, Accuracy: 9839/10000 (98%)\n",
      "\n",
      "Train: [48640/60000(81%)]\tLoss: 0.040473\n",
      "\n",
      "Test set: Average loss: 0.0496, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train: [49280/60000(82%)]\tLoss: 0.057051\n",
      "\n",
      "Test set: Average loss: 0.0484, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Train: [49920/60000(83%)]\tLoss: 0.078561\n",
      "\n",
      "Test set: Average loss: 0.0494, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Train: [50560/60000(84%)]\tLoss: 0.052383\n",
      "\n",
      "Test set: Average loss: 0.0528, Accuracy: 9822/10000 (98%)\n",
      "\n",
      "Train: [51200/60000(85%)]\tLoss: 0.068886\n",
      "\n",
      "Test set: Average loss: 0.0574, Accuracy: 9813/10000 (98%)\n",
      "\n",
      "Train: [51840/60000(86%)]\tLoss: 0.036204\n",
      "\n",
      "Test set: Average loss: 0.0519, Accuracy: 9832/10000 (98%)\n",
      "\n",
      "Train: [52480/60000(87%)]\tLoss: 0.010114\n",
      "\n",
      "Test set: Average loss: 0.0476, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train: [53120/60000(88%)]\tLoss: 0.054176\n",
      "\n",
      "Test set: Average loss: 0.0480, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train: [53760/60000(90%)]\tLoss: 0.046480\n",
      "\n",
      "Test set: Average loss: 0.0519, Accuracy: 9826/10000 (98%)\n",
      "\n",
      "Train: [54400/60000(91%)]\tLoss: 0.012727\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 9845/10000 (98%)\n",
      "\n",
      "Train: [55040/60000(92%)]\tLoss: 0.013882\n",
      "\n",
      "Test set: Average loss: 0.0482, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train: [55680/60000(93%)]\tLoss: 0.146322\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 9809/10000 (98%)\n",
      "\n",
      "Train: [56320/60000(94%)]\tLoss: 0.040592\n",
      "\n",
      "Test set: Average loss: 0.0481, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train: [56960/60000(95%)]\tLoss: 0.015167\n",
      "\n",
      "Test set: Average loss: 0.0537, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "Train: [57600/60000(96%)]\tLoss: 0.018184\n",
      "\n",
      "Test set: Average loss: 0.0479, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Train: [58240/60000(97%)]\tLoss: 0.073936\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train: [58880/60000(98%)]\tLoss: 0.089880\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Train: [59520/60000(99%)]\tLoss: 0.007808\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 9850/10000 (98%)\n",
      "\n",
      "Model trained with adagrad saved as 'mnist_cnn_adagrad.pt'\n",
      "\n",
      "Training with rmsprop optimizer:\n",
      "Train: [0/60000(0%)]\tLoss: 2.301589\n",
      "\n",
      "Test set: Average loss: 3350.9872, Accuracy: 1009/10000 (10%)\n",
      "\n",
      "Train: [640/60000(1%)]\tLoss: 2.371510\n",
      "\n",
      "Test set: Average loss: 2.3126, Accuracy: 950/10000 (10%)\n",
      "\n",
      "Train: [1280/60000(2%)]\tLoss: 2.277827\n",
      "\n",
      "Test set: Average loss: 2.2567, Accuracy: 2328/10000 (23%)\n",
      "\n",
      "Train: [1920/60000(3%)]\tLoss: 1.902490\n",
      "\n",
      "Test set: Average loss: 1.9175, Accuracy: 3042/10000 (30%)\n",
      "\n",
      "Train: [2560/60000(4%)]\tLoss: 0.768337\n",
      "\n",
      "Test set: Average loss: 0.8545, Accuracy: 7653/10000 (77%)\n",
      "\n",
      "Train: [3200/60000(5%)]\tLoss: 0.534812\n",
      "\n",
      "Test set: Average loss: 0.5423, Accuracy: 8425/10000 (84%)\n",
      "\n",
      "Train: [3840/60000(6%)]\tLoss: 0.427731\n",
      "\n",
      "Test set: Average loss: 0.4119, Accuracy: 8763/10000 (88%)\n",
      "\n",
      "Train: [4480/60000(7%)]\tLoss: 0.324231\n",
      "\n",
      "Test set: Average loss: 0.3480, Accuracy: 9028/10000 (90%)\n",
      "\n",
      "Train: [5120/60000(9%)]\tLoss: 0.302382\n",
      "\n",
      "Test set: Average loss: 0.3463, Accuracy: 8973/10000 (90%)\n",
      "\n",
      "Train: [5760/60000(10%)]\tLoss: 0.420061\n",
      "\n",
      "Test set: Average loss: 0.2837, Accuracy: 9169/10000 (92%)\n",
      "\n",
      "Train: [6400/60000(11%)]\tLoss: 0.241033\n",
      "\n",
      "Test set: Average loss: 0.2533, Accuracy: 9288/10000 (93%)\n",
      "\n",
      "Train: [7040/60000(12%)]\tLoss: 0.129456\n",
      "\n",
      "Test set: Average loss: 0.2541, Accuracy: 9270/10000 (93%)\n",
      "\n",
      "Train: [7680/60000(13%)]\tLoss: 0.243980\n",
      "\n",
      "Test set: Average loss: 0.2375, Accuracy: 9325/10000 (93%)\n",
      "\n",
      "Train: [8320/60000(14%)]\tLoss: 0.470634\n",
      "\n",
      "Test set: Average loss: 0.2855, Accuracy: 9129/10000 (91%)\n",
      "\n",
      "Train: [8960/60000(15%)]\tLoss: 0.480077\n",
      "\n",
      "Test set: Average loss: 0.3071, Accuracy: 9077/10000 (91%)\n",
      "\n",
      "Train: [9600/60000(16%)]\tLoss: 0.135192\n",
      "\n",
      "Test set: Average loss: 0.2118, Accuracy: 9398/10000 (94%)\n",
      "\n",
      "Train: [10240/60000(17%)]\tLoss: 0.304829\n",
      "\n",
      "Test set: Average loss: 0.2491, Accuracy: 9228/10000 (92%)\n",
      "\n",
      "Train: [10880/60000(18%)]\tLoss: 0.297548\n",
      "\n",
      "Test set: Average loss: 0.2230, Accuracy: 9363/10000 (94%)\n",
      "\n",
      "Train: [11520/60000(19%)]\tLoss: 0.345829\n",
      "\n",
      "Test set: Average loss: 0.1858, Accuracy: 9434/10000 (94%)\n",
      "\n",
      "Train: [12160/60000(20%)]\tLoss: 0.165706\n",
      "\n",
      "Test set: Average loss: 0.1947, Accuracy: 9399/10000 (94%)\n",
      "\n",
      "Train: [12800/60000(21%)]\tLoss: 0.218496\n",
      "\n",
      "Test set: Average loss: 0.1853, Accuracy: 9465/10000 (95%)\n",
      "\n",
      "Train: [13440/60000(22%)]\tLoss: 0.262179\n",
      "\n",
      "Test set: Average loss: 0.1933, Accuracy: 9429/10000 (94%)\n",
      "\n",
      "Train: [14080/60000(23%)]\tLoss: 0.308035\n",
      "\n",
      "Test set: Average loss: 0.1937, Accuracy: 9396/10000 (94%)\n",
      "\n",
      "Train: [14720/60000(25%)]\tLoss: 0.086850\n",
      "\n",
      "Test set: Average loss: 0.1894, Accuracy: 9452/10000 (95%)\n",
      "\n",
      "Train: [15360/60000(26%)]\tLoss: 0.075120\n",
      "\n",
      "Test set: Average loss: 0.2025, Accuracy: 9423/10000 (94%)\n",
      "\n",
      "Train: [16000/60000(27%)]\tLoss: 0.244703\n",
      "\n",
      "Test set: Average loss: 0.2564, Accuracy: 9289/10000 (93%)\n",
      "\n",
      "Train: [16640/60000(28%)]\tLoss: 0.127799\n",
      "\n",
      "Test set: Average loss: 0.1779, Accuracy: 9451/10000 (95%)\n",
      "\n",
      "Train: [17280/60000(29%)]\tLoss: 0.183662\n",
      "\n",
      "Test set: Average loss: 0.1886, Accuracy: 9413/10000 (94%)\n",
      "\n",
      "Train: [17920/60000(30%)]\tLoss: 0.060373\n",
      "\n",
      "Test set: Average loss: 0.1682, Accuracy: 9492/10000 (95%)\n",
      "\n",
      "Train: [18560/60000(31%)]\tLoss: 0.152056\n",
      "\n",
      "Test set: Average loss: 0.1741, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "Train: [19200/60000(32%)]\tLoss: 0.265701\n",
      "\n",
      "Test set: Average loss: 0.2874, Accuracy: 9147/10000 (91%)\n",
      "\n",
      "Train: [19840/60000(33%)]\tLoss: 0.258367\n",
      "\n",
      "Test set: Average loss: 0.2005, Accuracy: 9370/10000 (94%)\n",
      "\n",
      "Train: [20480/60000(34%)]\tLoss: 0.389650\n",
      "\n",
      "Test set: Average loss: 0.1902, Accuracy: 9443/10000 (94%)\n",
      "\n",
      "Train: [21120/60000(35%)]\tLoss: 0.077824\n",
      "\n",
      "Test set: Average loss: 0.1881, Accuracy: 9451/10000 (95%)\n",
      "\n",
      "Train: [21760/60000(36%)]\tLoss: 0.054802\n",
      "\n",
      "Test set: Average loss: 0.1445, Accuracy: 9564/10000 (96%)\n",
      "\n",
      "Train: [22400/60000(37%)]\tLoss: 0.050491\n",
      "\n",
      "Test set: Average loss: 0.1526, Accuracy: 9525/10000 (95%)\n",
      "\n",
      "Train: [23040/60000(38%)]\tLoss: 0.391315\n",
      "\n",
      "Test set: Average loss: 0.1743, Accuracy: 9449/10000 (94%)\n",
      "\n",
      "Train: [23680/60000(39%)]\tLoss: 0.222147\n",
      "\n",
      "Test set: Average loss: 0.1553, Accuracy: 9524/10000 (95%)\n",
      "\n",
      "Train: [24320/60000(41%)]\tLoss: 0.281341\n",
      "\n",
      "Test set: Average loss: 0.1654, Accuracy: 9487/10000 (95%)\n",
      "\n",
      "Train: [24960/60000(42%)]\tLoss: 0.322606\n",
      "\n",
      "Test set: Average loss: 0.2175, Accuracy: 9318/10000 (93%)\n",
      "\n",
      "Train: [25600/60000(43%)]\tLoss: 0.347860\n",
      "\n",
      "Test set: Average loss: 0.1938, Accuracy: 9395/10000 (94%)\n",
      "\n",
      "Train: [26240/60000(44%)]\tLoss: 0.110225\n",
      "\n",
      "Test set: Average loss: 0.1880, Accuracy: 9464/10000 (95%)\n",
      "\n",
      "Train: [26880/60000(45%)]\tLoss: 0.421752\n",
      "\n",
      "Test set: Average loss: 0.1773, Accuracy: 9478/10000 (95%)\n",
      "\n",
      "Train: [27520/60000(46%)]\tLoss: 0.124744\n",
      "\n",
      "Test set: Average loss: 0.1403, Accuracy: 9565/10000 (96%)\n",
      "\n",
      "Train: [28160/60000(47%)]\tLoss: 0.048702\n",
      "\n",
      "Test set: Average loss: 0.1486, Accuracy: 9549/10000 (95%)\n",
      "\n",
      "Train: [28800/60000(48%)]\tLoss: 0.176685\n",
      "\n",
      "Test set: Average loss: 0.1644, Accuracy: 9492/10000 (95%)\n",
      "\n",
      "Train: [29440/60000(49%)]\tLoss: 0.272779\n",
      "\n",
      "Test set: Average loss: 0.2047, Accuracy: 9366/10000 (94%)\n",
      "\n",
      "Train: [30080/60000(50%)]\tLoss: 0.043465\n",
      "\n",
      "Test set: Average loss: 0.1715, Accuracy: 9470/10000 (95%)\n",
      "\n",
      "Train: [30720/60000(51%)]\tLoss: 0.162522\n",
      "\n",
      "Test set: Average loss: 0.1629, Accuracy: 9480/10000 (95%)\n",
      "\n",
      "Train: [31360/60000(52%)]\tLoss: 0.054304\n",
      "\n",
      "Test set: Average loss: 0.1744, Accuracy: 9465/10000 (95%)\n",
      "\n",
      "Train: [32000/60000(53%)]\tLoss: 0.200385\n",
      "\n",
      "Test set: Average loss: 0.2300, Accuracy: 9340/10000 (93%)\n",
      "\n",
      "Train: [32640/60000(54%)]\tLoss: 0.252108\n",
      "\n",
      "Test set: Average loss: 0.1689, Accuracy: 9484/10000 (95%)\n",
      "\n",
      "Train: [33280/60000(55%)]\tLoss: 0.216243\n",
      "\n",
      "Test set: Average loss: 0.1480, Accuracy: 9553/10000 (96%)\n",
      "\n",
      "Train: [33920/60000(57%)]\tLoss: 0.397936\n",
      "\n",
      "Test set: Average loss: 0.2208, Accuracy: 9411/10000 (94%)\n",
      "\n",
      "Train: [34560/60000(58%)]\tLoss: 0.199299\n",
      "\n",
      "Test set: Average loss: 0.1744, Accuracy: 9486/10000 (95%)\n",
      "\n",
      "Train: [35200/60000(59%)]\tLoss: 0.035266\n",
      "\n",
      "Test set: Average loss: 0.1557, Accuracy: 9544/10000 (95%)\n",
      "\n",
      "Train: [35840/60000(60%)]\tLoss: 0.303078\n",
      "\n",
      "Test set: Average loss: 0.1572, Accuracy: 9515/10000 (95%)\n",
      "\n",
      "Train: [36480/60000(61%)]\tLoss: 0.182356\n",
      "\n",
      "Test set: Average loss: 0.1436, Accuracy: 9584/10000 (96%)\n",
      "\n",
      "Train: [37120/60000(62%)]\tLoss: 0.318254\n",
      "\n",
      "Test set: Average loss: 0.2648, Accuracy: 9178/10000 (92%)\n",
      "\n",
      "Train: [37760/60000(63%)]\tLoss: 0.283773\n",
      "\n",
      "Test set: Average loss: 0.1609, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "Train: [38400/60000(64%)]\tLoss: 0.109619\n",
      "\n",
      "Test set: Average loss: 0.1463, Accuracy: 9569/10000 (96%)\n",
      "\n",
      "Train: [39040/60000(65%)]\tLoss: 0.067658\n",
      "\n",
      "Test set: Average loss: 0.1767, Accuracy: 9499/10000 (95%)\n",
      "\n",
      "Train: [39680/60000(66%)]\tLoss: 0.014008\n",
      "\n",
      "Test set: Average loss: 0.1744, Accuracy: 9523/10000 (95%)\n",
      "\n",
      "Train: [40320/60000(67%)]\tLoss: 0.079530\n",
      "\n",
      "Test set: Average loss: 0.1248, Accuracy: 9613/10000 (96%)\n",
      "\n",
      "Train: [40960/60000(68%)]\tLoss: 0.221425\n",
      "\n",
      "Test set: Average loss: 0.1517, Accuracy: 9519/10000 (95%)\n",
      "\n",
      "Train: [41600/60000(69%)]\tLoss: 0.435423\n",
      "\n",
      "Test set: Average loss: 0.2683, Accuracy: 9200/10000 (92%)\n",
      "\n",
      "Train: [42240/60000(70%)]\tLoss: 0.363588\n",
      "\n",
      "Test set: Average loss: 0.1906, Accuracy: 9441/10000 (94%)\n",
      "\n",
      "Train: [42880/60000(71%)]\tLoss: 0.127277\n",
      "\n",
      "Test set: Average loss: 0.1316, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "Train: [43520/60000(72%)]\tLoss: 0.124584\n",
      "\n",
      "Test set: Average loss: 0.1476, Accuracy: 9546/10000 (95%)\n",
      "\n",
      "Train: [44160/60000(74%)]\tLoss: 0.095906\n",
      "\n",
      "Test set: Average loss: 0.1474, Accuracy: 9572/10000 (96%)\n",
      "\n",
      "Train: [44800/60000(75%)]\tLoss: 0.042174\n",
      "\n",
      "Test set: Average loss: 0.1324, Accuracy: 9600/10000 (96%)\n",
      "\n",
      "Train: [45440/60000(76%)]\tLoss: 0.126678\n",
      "\n",
      "Test set: Average loss: 0.1733, Accuracy: 9469/10000 (95%)\n",
      "\n",
      "Train: [46080/60000(77%)]\tLoss: 0.156800\n",
      "\n",
      "Test set: Average loss: 0.1359, Accuracy: 9576/10000 (96%)\n",
      "\n",
      "Train: [46720/60000(78%)]\tLoss: 0.068800\n",
      "\n",
      "Test set: Average loss: 0.1487, Accuracy: 9576/10000 (96%)\n",
      "\n",
      "Train: [47360/60000(79%)]\tLoss: 0.134164\n",
      "\n",
      "Test set: Average loss: 0.1103, Accuracy: 9659/10000 (97%)\n",
      "\n",
      "Train: [48000/60000(80%)]\tLoss: 0.105752\n",
      "\n",
      "Test set: Average loss: 0.1096, Accuracy: 9659/10000 (97%)\n",
      "\n",
      "Train: [48640/60000(81%)]\tLoss: 0.237246\n",
      "\n",
      "Test set: Average loss: 0.1069, Accuracy: 9658/10000 (97%)\n",
      "\n",
      "Train: [49280/60000(82%)]\tLoss: 0.099652\n",
      "\n",
      "Test set: Average loss: 0.1424, Accuracy: 9568/10000 (96%)\n",
      "\n",
      "Train: [49920/60000(83%)]\tLoss: 0.157014\n",
      "\n",
      "Test set: Average loss: 0.1483, Accuracy: 9539/10000 (95%)\n",
      "\n",
      "Train: [50560/60000(84%)]\tLoss: 0.068220\n",
      "\n",
      "Test set: Average loss: 0.1233, Accuracy: 9650/10000 (96%)\n",
      "\n",
      "Train: [51200/60000(85%)]\tLoss: 0.268215\n",
      "\n",
      "Test set: Average loss: 0.1369, Accuracy: 9562/10000 (96%)\n",
      "\n",
      "Train: [51840/60000(86%)]\tLoss: 0.205727\n",
      "\n",
      "Test set: Average loss: 0.1373, Accuracy: 9575/10000 (96%)\n",
      "\n",
      "Train: [52480/60000(87%)]\tLoss: 0.121474\n",
      "\n",
      "Test set: Average loss: 0.2147, Accuracy: 9403/10000 (94%)\n",
      "\n",
      "Train: [53120/60000(88%)]\tLoss: 0.218653\n",
      "\n",
      "Test set: Average loss: 0.2013, Accuracy: 9408/10000 (94%)\n",
      "\n",
      "Train: [53760/60000(90%)]\tLoss: 0.176410\n",
      "\n",
      "Test set: Average loss: 0.1247, Accuracy: 9617/10000 (96%)\n",
      "\n",
      "Train: [54400/60000(91%)]\tLoss: 0.075079\n",
      "\n",
      "Test set: Average loss: 0.1005, Accuracy: 9685/10000 (97%)\n",
      "\n",
      "Train: [55040/60000(92%)]\tLoss: 0.169606\n",
      "\n",
      "Test set: Average loss: 0.1209, Accuracy: 9640/10000 (96%)\n",
      "\n",
      "Train: [55680/60000(93%)]\tLoss: 0.120076\n",
      "\n",
      "Test set: Average loss: 0.1108, Accuracy: 9656/10000 (97%)\n",
      "\n",
      "Train: [56320/60000(94%)]\tLoss: 0.094880\n",
      "\n",
      "Test set: Average loss: 0.1192, Accuracy: 9642/10000 (96%)\n",
      "\n",
      "Train: [56960/60000(95%)]\tLoss: 0.113654\n",
      "\n",
      "Test set: Average loss: 0.1098, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "Train: [57600/60000(96%)]\tLoss: 0.157237\n",
      "\n",
      "Test set: Average loss: 0.1215, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "Train: [58240/60000(97%)]\tLoss: 0.163173\n",
      "\n",
      "Test set: Average loss: 0.1143, Accuracy: 9630/10000 (96%)\n",
      "\n",
      "Train: [58880/60000(98%)]\tLoss: 0.117955\n",
      "\n",
      "Test set: Average loss: 0.1175, Accuracy: 9610/10000 (96%)\n",
      "\n",
      "Train: [59520/60000(99%)]\tLoss: 0.122009\n",
      "\n",
      "Test set: Average loss: 0.1327, Accuracy: 9621/10000 (96%)\n",
      "\n",
      "Model trained with rmsprop saved as 'mnist_cnn_rmsprop.pt'\n",
      "\n",
      "Training with adam optimizer:\n",
      "Train: [0/60000(0%)]\tLoss: 2.315070\n",
      "\n",
      "Test set: Average loss: 7.1481, Accuracy: 1535/10000 (15%)\n",
      "\n",
      "Train: [640/60000(1%)]\tLoss: 2.005283\n",
      "\n",
      "Test set: Average loss: 1.8950, Accuracy: 2922/10000 (29%)\n",
      "\n",
      "Train: [1280/60000(2%)]\tLoss: 0.951043\n",
      "\n",
      "Test set: Average loss: 0.8249, Accuracy: 7204/10000 (72%)\n",
      "\n",
      "Train: [1920/60000(3%)]\tLoss: 0.642036\n",
      "\n",
      "Test set: Average loss: 0.6736, Accuracy: 7688/10000 (77%)\n",
      "\n",
      "Train: [2560/60000(4%)]\tLoss: 0.484919\n",
      "\n",
      "Test set: Average loss: 0.5461, Accuracy: 8261/10000 (83%)\n",
      "\n",
      "Train: [3200/60000(5%)]\tLoss: 0.300923\n",
      "\n",
      "Test set: Average loss: 0.4986, Accuracy: 8420/10000 (84%)\n",
      "\n",
      "Train: [3840/60000(6%)]\tLoss: 0.326131\n",
      "\n",
      "Test set: Average loss: 0.4324, Accuracy: 8625/10000 (86%)\n",
      "\n",
      "Train: [4480/60000(7%)]\tLoss: 0.511443\n",
      "\n",
      "Test set: Average loss: 0.4091, Accuracy: 8721/10000 (87%)\n",
      "\n",
      "Train: [5120/60000(9%)]\tLoss: 0.417949\n",
      "\n",
      "Test set: Average loss: 0.4351, Accuracy: 8719/10000 (87%)\n",
      "\n",
      "Train: [5760/60000(10%)]\tLoss: 0.163488\n",
      "\n",
      "Test set: Average loss: 0.3945, Accuracy: 8830/10000 (88%)\n",
      "\n",
      "Train: [6400/60000(11%)]\tLoss: 0.376896\n",
      "\n",
      "Test set: Average loss: 0.3999, Accuracy: 8698/10000 (87%)\n",
      "\n",
      "Train: [7040/60000(12%)]\tLoss: 0.249533\n",
      "\n",
      "Test set: Average loss: 0.2946, Accuracy: 9099/10000 (91%)\n",
      "\n",
      "Train: [7680/60000(13%)]\tLoss: 0.336367\n",
      "\n",
      "Test set: Average loss: 0.3311, Accuracy: 8965/10000 (90%)\n",
      "\n",
      "Train: [8320/60000(14%)]\tLoss: 0.442773\n",
      "\n",
      "Test set: Average loss: 0.2906, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train: [8960/60000(15%)]\tLoss: 0.539316\n",
      "\n",
      "Test set: Average loss: 0.2993, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Train: [9600/60000(16%)]\tLoss: 0.254663\n",
      "\n",
      "Test set: Average loss: 0.3590, Accuracy: 8833/10000 (88%)\n",
      "\n",
      "Train: [10240/60000(17%)]\tLoss: 0.198412\n",
      "\n",
      "Test set: Average loss: 0.2826, Accuracy: 9122/10000 (91%)\n",
      "\n",
      "Train: [10880/60000(18%)]\tLoss: 0.291681\n",
      "\n",
      "Test set: Average loss: 0.2913, Accuracy: 9136/10000 (91%)\n",
      "\n",
      "Train: [11520/60000(19%)]\tLoss: 0.413605\n",
      "\n",
      "Test set: Average loss: 0.2825, Accuracy: 9154/10000 (92%)\n",
      "\n",
      "Train: [12160/60000(20%)]\tLoss: 0.510989\n",
      "\n",
      "Test set: Average loss: 0.2632, Accuracy: 9165/10000 (92%)\n",
      "\n",
      "Train: [12800/60000(21%)]\tLoss: 0.240513\n",
      "\n",
      "Test set: Average loss: 0.2363, Accuracy: 9268/10000 (93%)\n",
      "\n",
      "Train: [13440/60000(22%)]\tLoss: 0.249335\n",
      "\n",
      "Test set: Average loss: 0.2409, Accuracy: 9247/10000 (92%)\n",
      "\n",
      "Train: [14080/60000(23%)]\tLoss: 0.246888\n",
      "\n",
      "Test set: Average loss: 0.2276, Accuracy: 9311/10000 (93%)\n",
      "\n",
      "Train: [14720/60000(25%)]\tLoss: 0.206330\n",
      "\n",
      "Test set: Average loss: 0.2408, Accuracy: 9256/10000 (93%)\n",
      "\n",
      "Train: [15360/60000(26%)]\tLoss: 0.391341\n",
      "\n",
      "Test set: Average loss: 0.2578, Accuracy: 9204/10000 (92%)\n",
      "\n",
      "Train: [16000/60000(27%)]\tLoss: 0.322067\n",
      "\n",
      "Test set: Average loss: 0.2290, Accuracy: 9282/10000 (93%)\n",
      "\n",
      "Train: [16640/60000(28%)]\tLoss: 0.255745\n",
      "\n",
      "Test set: Average loss: 0.2471, Accuracy: 9229/10000 (92%)\n",
      "\n",
      "Train: [17280/60000(29%)]\tLoss: 0.255840\n",
      "\n",
      "Test set: Average loss: 0.2587, Accuracy: 9222/10000 (92%)\n",
      "\n",
      "Train: [17920/60000(30%)]\tLoss: 0.273184\n",
      "\n",
      "Test set: Average loss: 0.2410, Accuracy: 9215/10000 (92%)\n",
      "\n",
      "Train: [18560/60000(31%)]\tLoss: 0.151688\n",
      "\n",
      "Test set: Average loss: 0.2206, Accuracy: 9334/10000 (93%)\n",
      "\n",
      "Train: [19200/60000(32%)]\tLoss: 0.161480\n",
      "\n",
      "Test set: Average loss: 0.2386, Accuracy: 9277/10000 (93%)\n",
      "\n",
      "Train: [19840/60000(33%)]\tLoss: 0.239959\n",
      "\n",
      "Test set: Average loss: 0.2282, Accuracy: 9310/10000 (93%)\n",
      "\n",
      "Train: [20480/60000(34%)]\tLoss: 0.235439\n",
      "\n",
      "Test set: Average loss: 0.2409, Accuracy: 9243/10000 (92%)\n",
      "\n",
      "Train: [21120/60000(35%)]\tLoss: 0.112787\n",
      "\n",
      "Test set: Average loss: 0.2061, Accuracy: 9382/10000 (94%)\n",
      "\n",
      "Train: [21760/60000(36%)]\tLoss: 0.243216\n",
      "\n",
      "Test set: Average loss: 0.1949, Accuracy: 9369/10000 (94%)\n",
      "\n",
      "Train: [22400/60000(37%)]\tLoss: 0.064297\n",
      "\n",
      "Test set: Average loss: 0.1830, Accuracy: 9409/10000 (94%)\n",
      "\n",
      "Train: [23040/60000(38%)]\tLoss: 0.160506\n",
      "\n",
      "Test set: Average loss: 0.2116, Accuracy: 9362/10000 (94%)\n",
      "\n",
      "Train: [23680/60000(39%)]\tLoss: 0.160740\n",
      "\n",
      "Test set: Average loss: 0.2051, Accuracy: 9368/10000 (94%)\n",
      "\n",
      "Train: [24320/60000(41%)]\tLoss: 0.088925\n",
      "\n",
      "Test set: Average loss: 0.1873, Accuracy: 9412/10000 (94%)\n",
      "\n",
      "Train: [24960/60000(42%)]\tLoss: 0.181188\n",
      "\n",
      "Test set: Average loss: 0.1889, Accuracy: 9435/10000 (94%)\n",
      "\n",
      "Train: [25600/60000(43%)]\tLoss: 0.171437\n",
      "\n",
      "Test set: Average loss: 0.1984, Accuracy: 9407/10000 (94%)\n",
      "\n",
      "Train: [26240/60000(44%)]\tLoss: 0.185169\n",
      "\n",
      "Test set: Average loss: 0.1948, Accuracy: 9395/10000 (94%)\n",
      "\n",
      "Train: [26880/60000(45%)]\tLoss: 0.091535\n",
      "\n",
      "Test set: Average loss: 0.1874, Accuracy: 9421/10000 (94%)\n",
      "\n",
      "Train: [27520/60000(46%)]\tLoss: 0.255567\n",
      "\n",
      "Test set: Average loss: 0.1764, Accuracy: 9438/10000 (94%)\n",
      "\n",
      "Train: [28160/60000(47%)]\tLoss: 0.078103\n",
      "\n",
      "Test set: Average loss: 0.2098, Accuracy: 9294/10000 (93%)\n",
      "\n",
      "Train: [28800/60000(48%)]\tLoss: 0.154699\n",
      "\n",
      "Test set: Average loss: 0.2004, Accuracy: 9387/10000 (94%)\n",
      "\n",
      "Train: [29440/60000(49%)]\tLoss: 0.171099\n",
      "\n",
      "Test set: Average loss: 0.1710, Accuracy: 9494/10000 (95%)\n",
      "\n",
      "Train: [30080/60000(50%)]\tLoss: 0.189042\n",
      "\n",
      "Test set: Average loss: 0.1740, Accuracy: 9473/10000 (95%)\n",
      "\n",
      "Train: [30720/60000(51%)]\tLoss: 0.329043\n",
      "\n",
      "Test set: Average loss: 0.1676, Accuracy: 9480/10000 (95%)\n",
      "\n",
      "Train: [31360/60000(52%)]\tLoss: 0.135488\n",
      "\n",
      "Test set: Average loss: 0.1689, Accuracy: 9474/10000 (95%)\n",
      "\n",
      "Train: [32000/60000(53%)]\tLoss: 0.332385\n",
      "\n",
      "Test set: Average loss: 0.1730, Accuracy: 9513/10000 (95%)\n",
      "\n",
      "Train: [32640/60000(54%)]\tLoss: 0.228693\n",
      "\n",
      "Test set: Average loss: 0.1590, Accuracy: 9498/10000 (95%)\n",
      "\n",
      "Train: [33280/60000(55%)]\tLoss: 0.103295\n",
      "\n",
      "Test set: Average loss: 0.1833, Accuracy: 9435/10000 (94%)\n",
      "\n",
      "Train: [33920/60000(57%)]\tLoss: 0.158314\n",
      "\n",
      "Test set: Average loss: 0.1646, Accuracy: 9468/10000 (95%)\n",
      "\n",
      "Train: [34560/60000(58%)]\tLoss: 0.260647\n",
      "\n",
      "Test set: Average loss: 0.1878, Accuracy: 9431/10000 (94%)\n",
      "\n",
      "Train: [35200/60000(59%)]\tLoss: 0.184537\n",
      "\n",
      "Test set: Average loss: 0.1825, Accuracy: 9467/10000 (95%)\n",
      "\n",
      "Train: [35840/60000(60%)]\tLoss: 0.122408\n",
      "\n",
      "Test set: Average loss: 0.1820, Accuracy: 9426/10000 (94%)\n",
      "\n",
      "Train: [36480/60000(61%)]\tLoss: 0.119963\n",
      "\n",
      "Test set: Average loss: 0.1858, Accuracy: 9470/10000 (95%)\n",
      "\n",
      "Train: [37120/60000(62%)]\tLoss: 0.275840\n",
      "\n",
      "Test set: Average loss: 0.1950, Accuracy: 9366/10000 (94%)\n",
      "\n",
      "Train: [37760/60000(63%)]\tLoss: 0.122013\n",
      "\n",
      "Test set: Average loss: 0.1944, Accuracy: 9426/10000 (94%)\n",
      "\n",
      "Train: [38400/60000(64%)]\tLoss: 0.235366\n",
      "\n",
      "Test set: Average loss: 0.1770, Accuracy: 9447/10000 (94%)\n",
      "\n",
      "Train: [39040/60000(65%)]\tLoss: 0.069724\n",
      "\n",
      "Test set: Average loss: 0.2190, Accuracy: 9312/10000 (93%)\n",
      "\n",
      "Train: [39680/60000(66%)]\tLoss: 0.338808\n",
      "\n",
      "Test set: Average loss: 0.1689, Accuracy: 9485/10000 (95%)\n",
      "\n",
      "Train: [40320/60000(67%)]\tLoss: 0.147456\n",
      "\n",
      "Test set: Average loss: 0.1498, Accuracy: 9538/10000 (95%)\n",
      "\n",
      "Train: [40960/60000(68%)]\tLoss: 0.138121\n",
      "\n",
      "Test set: Average loss: 0.1591, Accuracy: 9513/10000 (95%)\n",
      "\n",
      "Train: [41600/60000(69%)]\tLoss: 0.095898\n",
      "\n",
      "Test set: Average loss: 0.1614, Accuracy: 9494/10000 (95%)\n",
      "\n",
      "Train: [42240/60000(70%)]\tLoss: 0.111402\n",
      "\n",
      "Test set: Average loss: 0.1466, Accuracy: 9540/10000 (95%)\n",
      "\n",
      "Train: [42880/60000(71%)]\tLoss: 0.105483\n",
      "\n",
      "Test set: Average loss: 0.1640, Accuracy: 9496/10000 (95%)\n",
      "\n",
      "Train: [43520/60000(72%)]\tLoss: 0.158492\n",
      "\n",
      "Test set: Average loss: 0.1923, Accuracy: 9413/10000 (94%)\n",
      "\n",
      "Train: [44160/60000(74%)]\tLoss: 0.270258\n",
      "\n",
      "Test set: Average loss: 0.1663, Accuracy: 9486/10000 (95%)\n",
      "\n",
      "Train: [44800/60000(75%)]\tLoss: 0.109017\n",
      "\n",
      "Test set: Average loss: 0.1896, Accuracy: 9472/10000 (95%)\n",
      "\n",
      "Train: [45440/60000(76%)]\tLoss: 0.177116\n",
      "\n",
      "Test set: Average loss: 0.2022, Accuracy: 9386/10000 (94%)\n",
      "\n",
      "Train: [46080/60000(77%)]\tLoss: 0.273030\n",
      "\n",
      "Test set: Average loss: 0.1662, Accuracy: 9468/10000 (95%)\n",
      "\n",
      "Train: [46720/60000(78%)]\tLoss: 0.144881\n",
      "\n",
      "Test set: Average loss: 0.1617, Accuracy: 9503/10000 (95%)\n",
      "\n",
      "Train: [47360/60000(79%)]\tLoss: 0.103183\n",
      "\n",
      "Test set: Average loss: 0.1726, Accuracy: 9468/10000 (95%)\n",
      "\n",
      "Train: [48000/60000(80%)]\tLoss: 0.164906\n",
      "\n",
      "Test set: Average loss: 0.1558, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Train: [48640/60000(81%)]\tLoss: 0.158069\n",
      "\n",
      "Test set: Average loss: 0.1516, Accuracy: 9499/10000 (95%)\n",
      "\n",
      "Train: [49280/60000(82%)]\tLoss: 0.044280\n",
      "\n",
      "Test set: Average loss: 0.1557, Accuracy: 9530/10000 (95%)\n",
      "\n",
      "Train: [49920/60000(83%)]\tLoss: 0.121417\n",
      "\n",
      "Test set: Average loss: 0.1478, Accuracy: 9532/10000 (95%)\n",
      "\n",
      "Train: [50560/60000(84%)]\tLoss: 0.162321\n",
      "\n",
      "Test set: Average loss: 0.1570, Accuracy: 9510/10000 (95%)\n",
      "\n",
      "Train: [51200/60000(85%)]\tLoss: 0.163365\n",
      "\n",
      "Test set: Average loss: 0.1496, Accuracy: 9526/10000 (95%)\n",
      "\n",
      "Train: [51840/60000(86%)]\tLoss: 0.150477\n",
      "\n",
      "Test set: Average loss: 0.1347, Accuracy: 9600/10000 (96%)\n",
      "\n",
      "Train: [52480/60000(87%)]\tLoss: 0.213712\n",
      "\n",
      "Test set: Average loss: 0.1314, Accuracy: 9584/10000 (96%)\n",
      "\n",
      "Train: [53120/60000(88%)]\tLoss: 0.042223\n",
      "\n",
      "Test set: Average loss: 0.1450, Accuracy: 9550/10000 (96%)\n",
      "\n",
      "Train: [53760/60000(90%)]\tLoss: 0.095297\n",
      "\n",
      "Test set: Average loss: 0.1352, Accuracy: 9580/10000 (96%)\n",
      "\n",
      "Train: [54400/60000(91%)]\tLoss: 0.176176\n",
      "\n",
      "Test set: Average loss: 0.1571, Accuracy: 9529/10000 (95%)\n",
      "\n",
      "Train: [55040/60000(92%)]\tLoss: 0.211104\n",
      "\n",
      "Test set: Average loss: 0.1280, Accuracy: 9601/10000 (96%)\n",
      "\n",
      "Train: [55680/60000(93%)]\tLoss: 0.081425\n",
      "\n",
      "Test set: Average loss: 0.1707, Accuracy: 9470/10000 (95%)\n",
      "\n",
      "Train: [56320/60000(94%)]\tLoss: 0.138322\n",
      "\n",
      "Test set: Average loss: 0.1564, Accuracy: 9523/10000 (95%)\n",
      "\n",
      "Train: [56960/60000(95%)]\tLoss: 0.322163\n",
      "\n",
      "Test set: Average loss: 0.1364, Accuracy: 9573/10000 (96%)\n",
      "\n",
      "Train: [57600/60000(96%)]\tLoss: 0.186317\n",
      "\n",
      "Test set: Average loss: 0.1418, Accuracy: 9564/10000 (96%)\n",
      "\n",
      "Train: [58240/60000(97%)]\tLoss: 0.061367\n",
      "\n",
      "Test set: Average loss: 0.1369, Accuracy: 9595/10000 (96%)\n",
      "\n",
      "Train: [58880/60000(98%)]\tLoss: 0.283752\n",
      "\n",
      "Test set: Average loss: 0.1806, Accuracy: 9412/10000 (94%)\n",
      "\n",
      "Train: [59520/60000(99%)]\tLoss: 0.079704\n",
      "\n",
      "Test set: Average loss: 0.1391, Accuracy: 9584/10000 (96%)\n",
      "\n",
      "Model trained with adam saved as 'mnist_cnn_adam.pt'\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
    "train_kwargs.update(cuda_kwargs)\n",
    "test_kwargs.update(cuda_kwargs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "# Initialize the model and optimizer list\n",
    "log_interval = 10\n",
    "\n",
    "# Create the optimizer dictionary\n",
    "optimizers = {\n",
    "    \"adagrad\": adagrad_optimizer(Net().to(device), lr=0.01, epsilon=1e-6),\n",
    "    \"rmsprop\": rmsprop_optimizer(Net().to(device), lr=0.01, rho=0.99, epsilon=1e-6),\n",
    "    \"adam\": adam_optimizer(Net().to(device), lr=0.01, beta1=0.9, beta2=0.99, epsilon=1e-6)\n",
    "}\n",
    "\n",
    "# Train each optimizer separately and save the model\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    print(f\"\\nTraining with {optimizer_name} optimizer:\")\n",
    "\n",
    "    # Initializing the model\n",
    "    model = optimizer.model\n",
    "\n",
    "    # training model\n",
    "    train(log_interval, model, device, train_loader, test_loader, optimizer)\n",
    "\n",
    "    # save\n",
    "    torch.save(model.state_dict(), f\"mnist_cnn_{optimizer_name}.pt\")\n",
    "    print(f\"Model trained with {optimizer_name} saved as 'mnist_cnn_{optimizer_name}.pt'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
